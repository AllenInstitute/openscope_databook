{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20be5b75",
   "metadata": {},
   "source": [
    "# Generalized Linear Models using Pynapple & NeMos\n",
    "In this notebook, we will use NeMos and Pynapple packages (supported by the [Flatiron Institute](https://neurorse.flatironinstitute.org)), to model spiking neural data using [Generalized Linear Models (GLM)](https://en.wikipedia.org/wiki/Generalized_linear_model). We will explain what GLMs are and which are their components, then use [Pynapple](https://pynapple.org) and [NeMos](https://nemos.readthedocs.io/en/latest/) python packages to preprocess real data from the Primary Visual Cortex (VISp) of mice, and use a GLM model to predict spiking neural data as a function of passive visual stimuli. Moreover, we will also compare this model including an extra predictor: spike history, to assess whether adding history as a predictor improves the performance.\n",
    "\n",
    "We will be analyzing data from the [Visual Coding - Neuropixels dataset](https://portal.brain-map.org/circuits-behavior/visual-coding-neuropixels), published by the Allen Institute. This dataset uses [extracellular electrophysiology probes](https://www.nature.com/articles/nature24636) to record spikes from multiple regions in the brain during passive visual stimulation. For simplicity, we will focus on the activity of neurons in the visual cortex (VISp) during passive exposure to full-field flashes of color either black (coded as \"-1.0\") or white (coded as \"1.0\") in a gray background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03d635",
   "metadata": {},
   "source": [
    "### Background on GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc87c4",
   "metadata": {},
   "source": [
    "\n",
    "A GLM is a regression model which trains a filter to predict a value (output) as it relates to some other variable (or input). In the neuroscience context, we can use a particular type of GLM to predict spikes: the linear-nonlinear-Poisson (LNP) model. This type of model receives one or more inputs and then sends them through a linear  \"filter\" or transformation, passes said transformation through a nonlinearity to get the firing rate and uses that firing rate as the mean of a Poisson distribution to generate spikes. \n",
    "\n",
    "![LNP model schematic](../../data/images/lnp_model.png)\n",
    "<p align=\"center\">\n",
    "LNP model schematic. Modified from <a href=\"https://www.nature.com/articles/nature07140\">Pillow et al., 2008</a>\n",
    "</p>\n",
    "\n",
    "1. Sends the inputs through a linear \"filter\" or transformation\n",
    "     \n",
    "    The inputs (also known as \"predictors\") are first passed through a linear transformation:\n",
    "    \n",
    "    $\n",
    "    \\begin{aligned}\n",
    "    L(X) = WX + c\n",
    "    \\end{aligned}\n",
    "    $\n",
    "\n",
    "    Where $X$ is the input (in matrix form), $W$ is a matrix and $c$ is a vector (intercept).\n",
    "\n",
    "    $L$ scales (makes bigger or smaller) or shifts (up or down) the input. When there is zero input, this is equivalent to changing the baseline rate of the neuron, which is how the intercept should be interpreted. So far, this is the same treatment of an ordinary linear regression. \n",
    "\n",
    "2. Passes the transformation through a nonlinearity to get the firing rate.\n",
    "    \n",
    "    The aim of a LNP model is to predict the firing rate of a neuron and use it to generate spikes, but if we were only to keep $L(X)$ as it is, we would quickly notice that we could obtain negative values for firing rates, which makes no sense! This is what the nonlinearity part of the model handles: by passing the linear transformation through an exponential function, it is assured that the resulting firing rate will always be non-negative. \n",
    "\n",
    "    As such, the firing rate in a LNP model is defined:\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\lambda =  exp(L(X))\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    where $\\lambda$ is a vector containing the firing rates corresponding to each timepoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ef026",
   "metadata": {},
   "source": [
    ":::{admonition} A note on nonlinearity\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "\n",
    "In NeMoS, the nonlinearity is kept fixed. We default to the exponential, but a small number of other choices, such as soft-plus, are allowed. The allowed choices guarantee both the non-negativity constraint described above, as well as convexity, i.e. a single optimal solution. In principle, one could choose a more complex nonlinearity, but convexity is not guaranteed in general.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32d362",
   "metadata": {},
   "source": [
    ":::{admonition} What is the difference between a \"link function\" and the \"nonlinearity\"?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "The link function states the relationship between the linear predictor and the mean of the distribution function. If $g$ is a link function, $L(⋅)$ is the linear predictor and $\\lambda$ the mean of the distribution function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g(\\lambda) = L(⋅)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\lambda = g^{-1}(L(⋅))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "the \"nonlinearity\" is the name for the inverse of the link function $g^{-1}(⋅)$.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5557370",
   "metadata": {},
   "source": [
    "3. Uses the firing rate as the mean of a Poisson distribution to generate spikes\n",
    "\n",
    "    In this type of GLM, each spike train is modeled as a sample from a Poisson distribution whose mean is the firing rate — that is, the output of the linear-nonlinear components of the model.\n",
    "\n",
    "    Spiking is a stochastic process. This means that a given firing rate can lead to many different possible spike trains. Since the model could generate an infinite number of spike train realizations, how do we evaluate how well it explains the single observed spike train? We do this by computing the log-likelihood: it quantifies how likely it is to observe the actual spike train given the predicted firing rate. If $ y(t) $ is the observed spike count and $ \\lambda(t) $ is the predicted firing rate at time $ t $, then the log-likelihood at time $ t $:\n",
    "\n",
    "    $$\n",
    "    \\log P(y(t) \\mid \\lambda(t)) = y(t)\\log\\lambda(t) - \\lambda(t) -\\log(y(t)!)\n",
    "    $$\n",
    "\n",
    "    However, the term $ -\\log(y(t)!) $ does not depend on $ \\lambda $, and therefore is constant with respect to the model. As a result, it is usually dropped during optimization, leaving us with the simplified log-likelihood:\n",
    "\n",
    "    $$\n",
    "    \\log P(y(t) \\mid \\lambda(t)) = y(t) \\log \\lambda(t) - \\lambda(t)\n",
    "    $$\n",
    "\n",
    "    This forms the loss function for LNPs. In practice, we aim to maximize this log-likelihood, which is equivalent to minimizing the negative log-likelihood — that is, finding the firing rate $\\lambda(t)$ that makes the observed spike train as likely as possible under the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bb322",
   "metadata": {},
   "source": [
    ":::{admonition} Why using GLMs?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "1. Why not just use linear regression? Because neural data breaks its key assumptions. Linear regression expects normally distributed data with constant variance, but spike counts are non-Gaussian. Even more problematic, neural variability isn't constant: neurons that fire more frequently also tend to be more variable. This violates the homoscedasticity assumption that's fundamental to linear regression, making GLMs a much more suitable framework for modeling neural activity.\n",
    "\n",
    "2. GLMs are as easy to fit as linear regression! The objective function (negative log-likelihood) of GLMs with canonical link functions (such as log link which we are using here) is convex, which means there is one local minimum and no local maxima, ensuring convergence to the right answer.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf8020",
   "metadata": {},
   "source": [
    ":::{admonition} More resources on GLMs\n",
    ":class: seealso\n",
    ":class: dropdown\n",
    "\n",
    "If you would like to learn more about GLMs, you can refer to:\n",
    "\n",
    "- [Nemos GLM tutorial](https://nemos.readthedocs.io/en/latest/background/plot_00_conceptual_intro.html): for a bit more detailed explanation of all the components of a GLM within the NEMOS framework, as well as some nice visualizations of all the steps of the input transformation!\n",
    "- [Neuromatch Academy GLM tutorial](https://compneuro.neuromatch.io/tutorials/W1D3_GeneralizedLinearModels/student/W1D3_Tutorial1.html): for a bit  more detailed explanation of the components of a GLM, slides and some coding exercises to ensure comprehension.\n",
    "- [Jonathan Pillow's COSYNE tutorial](https://www.youtube.com/watch?v=NFeGW5ljUoI&t=4230s): for a longer tutorial of all of the components of a GLM, as well as different types of GLM besides LNP\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2830ac",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "- Recognize how to structure data for NeMos using Pynapple\n",
    "- Recognize how to fit a basic GLM using NeMos\n",
    "- Recognize basis objects, and explain their usefulness\n",
    "- Recognize the important terminology necessary for fitting (regularization, tolerance, cross-validation)\n",
    "- Recognize how to retrieve the parameters and predictions from a fit GLM for interpretation\n",
    "- Recognize how to evaluate the performance of a model, and compare models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662ddc7",
   "metadata": {},
   "source": [
    "### Create Environment and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb34563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    from databook_utils.dandi_utils import dandi_download_open\\nexcept:\\n    !git clone https://github.com/AllenInstitute/openscope_databook.git\\n    %cd openscope_databook\\n    %pip install -e .\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install requirements for the databook\n",
    "'''\n",
    "try:\n",
    "    from databook_utils.dandi_utils import dandi_download_open\n",
    "except:\n",
    "    !git clone https://github.com/AllenInstitute/openscope_databook.git\n",
    "    %cd openscope_databook\n",
    "    %pip install -e .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b541f8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%pip install \"pooch>=1.8.2\"\\n%pip install \"dandi>=0.67.2\"\\n%pip install \"jaxopt>=0.8.5\"\\n%pip install \"nemos>=0.2.3\"\\n%pip install \"pynapple>=0.9.2\"\\n%pip install \"seaborn>=0.13.2\"\\n%pip install \"scikit-learn>=1.7.0\"\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install requirements for this notebook\n",
    "'''\n",
    "%pip install \"pooch>=1.8.2\"\n",
    "%pip install \"dandi>=0.67.2\"\n",
    "%pip install \"jaxopt>=0.8.5\"\n",
    "%pip install \"nemos>=0.2.3\"\n",
    "%pip install \"pynapple>=0.9.2\"\n",
    "%pip install \"seaborn>=0.13.2\"\n",
    "%pip install \"scikit-learn>=1.7.0\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05db4790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uv pip install \"pooch>=1.8.2\"\\nuv pip install \"dandi>=0.67.2\"\\nuv pip install \"jaxopt>=0.8.5\"\\nuv pip install \"nemos>=0.2.3\"\\nuv pip install \"pynapple>=0.9.2\"\\nuv pip install \"seaborn>=0.13.2\"\\nuv pip install \"scikit-learn>=1.7.0\"\\nuv pip install \"ipykernel>=6.29.5\"\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''uv pip install \"pooch>=1.8.2\"\n",
    "uv pip install \"dandi>=0.67.2\"\n",
    "uv pip install \"jaxopt>=0.8.5\"\n",
    "uv pip install \"nemos>=0.2.3\"\n",
    "uv pip install \"pynapple>=0.9.2\"\n",
    "uv pip install \"seaborn>=0.13.2\"\n",
    "uv pip install \"scikit-learn>=1.7.0\"\n",
    "uv pip install \"ipykernel>=6.29.5\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccad111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pynapple as nap\n",
    "\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import zscore\n",
    "import itertools\n",
    "import nemos as nmo\n",
    "from nemos.basis import RaisedCosineLinearConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddde7d01",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for plotting\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be616aaa",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1de063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "dandiset_id = \"000021\"\n",
    "dandi_filepath = \"sub-726298249/sub-726298249_ses-754829445.nwb\"\n",
    "download_loc = \".\"\n",
    "\n",
    "# Download using nemos\n",
    "io = nmo.fetch.download_dandi_data(dandiset_id, dandi_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340622b3",
   "metadata": {},
   "source": [
    "Now that we have downloaded the data, it is very simple to open the dataset with Pynapple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a338af00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754829445\n",
      "┍━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━┑\n",
      "│ Keys                                               │ Type        │\n",
      "┝━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━┥\n",
      "│ units                                              │ TsGroup     │\n",
      "│ static_gratings_presentations                      │ IntervalSet │\n",
      "│ spontaneous_presentations                          │ IntervalSet │\n",
      "│ natural_scenes_presentations                       │ IntervalSet │\n",
      "│ natural_movie_three_presentations                  │ IntervalSet │\n",
      "│ natural_movie_one_presentations                    │ IntervalSet │\n",
      "│ gabors_presentations                               │ IntervalSet │\n",
      "│ flashes_presentations                              │ IntervalSet │\n",
      "│ drifting_gratings_presentations                    │ IntervalSet │\n",
      "│ timestamps                                         │ Tsd         │\n",
      "│ running_wheel_rotation                             │ Tsd         │\n",
      "│ running_speed_end_times                            │ Tsd         │\n",
      "│ running_speed                                      │ Tsd         │\n",
      "│ raw_gaze_mapping/screen_coordinates_spherical      │ TsdFrame    │\n",
      "│ raw_gaze_mapping/screen_coordinates                │ TsdFrame    │\n",
      "│ raw_gaze_mapping/pupil_area                        │ Tsd         │\n",
      "│ raw_gaze_mapping/eye_area                          │ Tsd         │\n",
      "│ optogenetic_stimulation                            │ IntervalSet │\n",
      "│ optotagging                                        │ Tsd         │\n",
      "│ filtered_gaze_mapping/screen_coordinates_spherical │ TsdFrame    │\n",
      "│ filtered_gaze_mapping/screen_coordinates           │ TsdFrame    │\n",
      "│ filtered_gaze_mapping/pupil_area                   │ Tsd         │\n",
      "│ filtered_gaze_mapping/eye_area                     │ Tsd         │\n",
      "│ running_wheel_supply_voltage                       │ Tsd         │\n",
      "│ running_wheel_signal_voltage                       │ Tsd         │\n",
      "│ raw_running_wheel_rotation                         │ Tsd         │\n",
      "┕━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━┙\n"
     ]
    }
   ],
   "source": [
    "data = nap.NWBFile(io.read(), lazy_loading=False)\n",
    "nwb = data.nwb\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ecc46c",
   "metadata": {},
   "source": [
    ":::{admonition} Pynapple objects\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "When printing data, we can see four type of Pynapple objects:\n",
    "- [```TsGroup```](https://pynapple.org/generated/pynapple.TsGroup.html#pynapple.TsGroup) : Dictionary-like object to group objects with different timestamps \n",
    "- [```IntervalSet```](https://pynapple.org/generated/pynapple.IntervalSet.html#pynapple.IntervalSet) : A class representing a (irregular) set of time intervals in elapsed time, with relative operations\n",
    "- [```Tsd```](https://pynapple.org/generated/pynapple.Tsd.html#pynapple.Tsd) : 1-dimensional container for neurophysiological time series - provides standardized time representation, plus various functions for manipulating times series.\n",
    "- [```TsdFrame```](https://pynapple.org/generated/pynapple.TsdFrame.html#pynapple.TsdFrame) : Column-based container for neurophysiological time series\n",
    "\n",
    "To learn more, please refer to the [Pynapple documentation](https://pynapple.org)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23870d",
   "metadata": {},
   "source": [
    "## Extraction, preprocessing and stimuli revision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e87f10",
   "metadata": {},
   "source": [
    "### Extracting Spiking Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070105d",
   "metadata": {},
   "source": [
    "We have a lot of information in ```data```, but we are interested in the units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e04e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = data[\"units\"]\n",
    "\n",
    "# See the columns\n",
    "print(f\"columns : {units.metadata_columns}\")\n",
    "\n",
    "# See the dataset\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fd330",
   "metadata": {},
   "source": [
    "Taking a closer look at the columns, we can see there is a lot of information we do not need. We are solely interested in predicting the spiking activity from the neurons from VISp. Thus, we will remove the metadata from all columns except for rate, quality (to make sure we filter the bad-quality neurons) and peak_channel_id (this last one contains relevant information for brain area identification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(cols_to_keep, data):\n",
    "    cols_to_remove = [col for col in data.metadata_columns if col not in cols_to_keep]\n",
    "    data.drop_info(cols_to_remove)\n",
    "    \n",
    "# Choose which columns to remove and remove them\n",
    "cols_to_keep = ['rate', 'quality','peak_channel_id']\n",
    "drop_cols(cols_to_keep,units)\n",
    "\n",
    "# See the dataset\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0970781",
   "metadata": {},
   "source": [
    "Here we dont have the brain area information but we need it, so we need to do some preprocessing to extract brain area from the nwb object using the peak_channel_id metadata. Luckily, Pynapple stored the nwb object as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units and brain areas those units belong to are in two different places. With the electrodes table, we can map units to their corresponding brain regions.\n",
    "def get_unit_location(unit_id):\n",
    "    \"\"\"Aligns location information from electrodes table with channel id from the units table\n",
    "    \"\"\"\n",
    "    return channel_probes[int(units[unit_id].peak_channel_id)]\n",
    "\n",
    "channel_probes = {}\n",
    "electrodes = nwb.electrodes\n",
    "\n",
    "for i in range(len(electrodes)):\n",
    "    channel_id = electrodes[\"id\"][i]\n",
    "    location = electrodes[\"location\"][i]\n",
    "    channel_probes[channel_id] = location\n",
    "\n",
    "# Add a new column to include location in our spikes TsGroup\n",
    "units.brain_area = [channel_probes[int(ch_id)] for ch_id in units.peak_channel_id]\n",
    "\n",
    "# Remove peak_channel_id because we already got the brain_area information\n",
    "units.drop_info(\"peak_channel_id\")\n",
    "\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596362f4",
   "metadata": {},
   "source": [
    "### Extracting stimulus data\n",
    "\n",
    "Mice were exposed to a series of stimuli (gabor patches, flashes, natural images, etc.), out of which we are exclusively interested in flashes presentation for this tutorial.\n",
    "\n",
    "![visual_stimuli_set.png](../../data/images/visual_stimuli_set.png)\n",
    "\n",
    "<p align=\"center\">\n",
    "Visual stimulus set. Modified from <a href=\"https://brainmapportal-live-4cc80a57cd6e400d854-f7fdcae.divio-media.net/filer_public/80/75/8075a100-ca64-429a-b39a-569121b612b2/neuropixels_visual_coding_-_white_paper_v10.pdf\">Allen Institute for Brain Science</a>\n",
    "</p>\n",
    "\n",
    "During the flashes presentation trials, mice were exposed to white or black full-field flashes in a gray background, each lasting 250 ms, and separated by a 2 second inter-trial interval. In total, they were exposed to 150 flashes (75 black, 75 white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract flashes as an Interval Set object\n",
    "flashes = data[\"flashes_presentations\"]\n",
    "\n",
    "# Remove unnecesary columns, similarly to above\n",
    "cols_to_keep = ['color']\n",
    "drop_cols(cols_to_keep, flashes)\n",
    "\n",
    "print(flashes)\n",
    "\n",
    "# Create an object for white and a separate object for black flashes\n",
    "flashes_white = flashes[flashes[\"color\"] == \"1.0\"]\n",
    "flashes_black = flashes[flashes[\"color\"] == \"-1.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8afc02",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_stimulus():\n",
    "    n_flashes = 5\n",
    "    n_seconds = 13\n",
    "    offset = .5\n",
    "\n",
    "    start = data[\"flashes_presentations\"][\"start\"].min() - offset\n",
    "    end = start + n_seconds\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (17, 4))\n",
    "    [ax.axvspan(s, e, color = \"silver\", alpha=.4, ec=\"black\") for s, e in zip(flashes_white[:n_flashes].start, flashes_white[:n_flashes].end)]\n",
    "    [ax.axvspan(s, e, color = \"black\", alpha=.4, ec=\"black\") for s, e in zip(flashes_black[:n_flashes].start, flashes_black[:n_flashes].end)]\n",
    "\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Absent = 0, Present = 1\")\n",
    "    ax.set_title(\"Stimulus presentation\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    plt.xlim(start-.1,end)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stimulus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3be63",
   "metadata": {},
   "source": [
    "### Preprocessing Spiking Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4c885",
   "metadata": {},
   "source": [
    "There are multiple reasons for filtering units. Here, we will use four criteria: brain area, quality of units, firing rate and responsiveness\n",
    "1. Brain area: we are interested in analyzing VISp units for this tutorial\n",
    "2. Quality: we will only select \"good\" quality units\n",
    "3. Firing rate: overall, we want units with a firing rate larger than 2 throughout all recordings\n",
    "4. Responsiveness: Since the goal is to predict firing rate as a function of stimuli, we are now specifically interested in how neurons respond to white and/or black flashes. To focus on meaningful signal, we will (1) exclude units that show no spiking activity during stimulus presentation. To be concise, (2) we will select the most responsive units (top 30% responsiveness), and only use those for further analysis. We define responsiveness as the normalized difference between post stimulus and pre stimulus average firing rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55075d44",
   "metadata": {},
   "source": [
    ":::{admonition} What does it mean for a unit to be of \"good\" quality?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "More information on unit quality metrics can be found in [Visualizing Unit Quality Metrics ](../visualization/visualize_unit_metrics.ipynb)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter units according 1, 2 and 3 criteria\n",
    "units = units[\n",
    "    (units[\"brain_area\"]==\"VISp\") & \n",
    "    (units[\"quality\"]==\"good\") & \n",
    "    (units[\"rate\"]>2.0)\n",
    "] \n",
    "print(units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e227567",
   "metadata": {},
   "source": [
    "Now, to calculate responsiveness, we need to do some preprocessing to align units spiking timestamps with the onset of the stimuli repetitions, and then take an average over the them. For this, we will use the [```compute_perievent```](https://pynapple.org/generated/pynapple.process.perievent.html#pynapple.process.perievent.compute_perievent) function, which allows to re-center time series and timestamps around particular events and compute spikes trigger average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eab1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set window of perievent 500ms before and after the start of the event\n",
    "window_size = (-.250, .500) \n",
    "\n",
    "# Calculate perievent for white stimuli\n",
    "peri_white = nap.compute_perievent(timestamps = units,\n",
    "                                        tref = flashes_white.starts,\n",
    "                                        minmax = window_size\n",
    ")\n",
    "\n",
    "# Calculate perievent for black stimuli\n",
    "peri_black = nap.compute_perievent(timestamps = units,\n",
    "                                        tref = flashes_black.starts,\n",
    "                                        minmax = window_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b070e6e",
   "metadata": {},
   "source": [
    "The output of the perievent is a dictionary of [```TsGroup```](https://pynapple.org/generated/pynapple.TsGroup.html#pynapple.TsGroup) objects, indexed by each unit ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09545a6b",
   "metadata": {},
   "source": [
    ":::{admonition} Will the output of ```compute_perievent``` always be a dictionary?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "No. In this case it is because the input was a ```TsGroup``` containing the spiking information of multiple units. Had it been a ```Ts/Tsd/TsdFrame/TsdTensor``` (only one unit), then the output of ```compute_perievent``` would have been a ```TsGroup```.\n",
    "For more information, please refer to [Pynapple documentation for processing perievents](https://pynapple.org/generated/pynapple.process.perievent.html#pynapple.process.perievent.compute_perievent)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568924b2",
   "metadata": {},
   "source": [
    "When we index one element of this dictionary, we can find the spike times centered around the stimulus for a single unit, which we could have changed with ```window_size```), for all presentations of stimuli. See the ref_times of the perievent are exactly the start times of stimuli presentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select one unit\n",
    "example_id = 951765547 \n",
    "print(f\"length of the TsGroup: {len(peri_black[example_id])}\\n \")\n",
    "\n",
    "# And print it's rates\n",
    "print(f\"perievent of unit {example_id}: \\n {peri_black[example_id]}\\n\")\n",
    "\n",
    "# Start times of black flashes presentation\n",
    "print(f\"black flashes start times: \\n {flashes_black.starts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61982b7",
   "metadata": {},
   "source": [
    "Let's inspect a bit further our perievents objects. If we grab the FIRST element of ```peri_black[example_id]```, we would get the spike times centered around the FIRST presentation of stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peri_black[example_id][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a7845e",
   "metadata": {},
   "source": [
    "It is expected to have negative spike times here because the spikes in ```peri_white``` and ```peri_black``` are centered around stimulus onset, so spikes which occured a bit before the onset would have \"negative\" values for time, as they occurred before $t=0$. Conversely, positive time values are indication of spikes that ocurred after stimulus onset.\n",
    "\n",
    "We can also plot these aligned spikes, to visualize the rate and timing of the spikes as they relate to the stimulus. This type of plot is named [Peri Event Histogram (PETH)](https://en.wikipedia.org/wiki/Peristimulus_time_histogram). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36409b03",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_raster_peth(peri_color, units, color_flashes, bin_sz, smoothing=0.015):\n",
    "    \"\"\"\n",
    "    Plot perievent time histograms (PETHs) and raster plots for multiple units.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    peri_color : dict\n",
    "        Dictionary mapping unit names to binned spike count peri-stimulus data (e.g., binned time series).\n",
    "    units : dict\n",
    "        Dictionary of neural units, e.g., spike trains or trial-aligned spike events.\n",
    "    color_flashes : str\n",
    "        A label indicating the flash color condition ('black' or other), used for visual styling.\n",
    "    bin_sz : float\n",
    "        Size of the bin used for spike count computation (in seconds).\n",
    "    smoothing : float\n",
    "        Standard deviation for Gaussian smoothing of the PETH traces.\n",
    "    \"\"\"\n",
    "\n",
    "    # Layout setup: 7 columns (units), 2 rows (split vertically into PETH and raster plot)\n",
    "    n_cols = 7\n",
    "    n_rows = 2\n",
    "    fig, ax = plt.subplots(n_rows, n_cols)\n",
    "    fig.set_figheight(4)\n",
    "    fig.set_figwidth(17)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Use tab10 color palette for plotting different units\n",
    "    colors = plt.cm.tab10.colors[:n_cols]\n",
    "\n",
    "    # Extract unit names for iteration\n",
    "    units_list = list(units.keys())[10:]\n",
    "\n",
    "    start = 0\n",
    "    end = int(n_rows / 2)  # Plot as many units as half the number of rows \n",
    "                            # each unit occupies 2 rows (one for peth and other for raster)\n",
    "\n",
    "    for col in range(n_cols):\n",
    "        for i, unit in enumerate(units_list[start:end]):\n",
    "            u = peri_color[unit]\n",
    "            line_color = colors[col]\n",
    "\n",
    "            # Plot PETH (smoothed firing rate)\n",
    "            ax[2*i, col].plot(\n",
    "                (np.mean(u.count(0.01), 1) / bin_sz).smooth(std=smoothing),\n",
    "                linewidth=2,\n",
    "                color=line_color\n",
    "            )\n",
    "            ax[2*i, col].axvline(0.0)  # Stimulus onset line\n",
    "            span_color = \"black\" if color_flashes == \"black\" else \"silver\"\n",
    "            ax[2*i, col].axvspan(0, 0.250, color=span_color, alpha=0.3, ec=\"black\")  # Stim duration\n",
    "            ax[2*i, col].set_xlim(-0.25, 0.50)\n",
    "            ax[2*i, col].set_title(f'{unit}')\n",
    "\n",
    "            # Plot raster\n",
    "            ax[2*i+1, col].plot(u.to_tsd(), \"|\", markersize=1, color=line_color, mew=2)\n",
    "            ax[2*i+1, col].axvline(0.0)\n",
    "            ax[2*i+1, col].axvspan(0, 0.250, color=span_color, alpha=0.3, ec=\"black\")\n",
    "            ax[2*i+1, col].set_ylim(0, 75)\n",
    "            ax[2*i+1, col].set_xlim(-0.25, 0.50)\n",
    "\n",
    "            # Shift window for next units\n",
    "            start += 1\n",
    "            end += 1\n",
    "\n",
    "    # Y-axis and title annotations\n",
    "    ax[0, 0].set_ylabel(\"Rate (Hz)\")\n",
    "    ax[1, 0].set_ylabel(\"Trial\")\n",
    "    if n_rows > 2:\n",
    "        ax[2, 0].set_ylabel(\"Rate (Hz)\")\n",
    "        ax[3, 0].set_ylabel(\"Trial\")\n",
    "    fig.text(0.5, 0.00, 'Time from stim(s)', ha='center')\n",
    "    fig.text(0.5, 1.00, f'PETH & Spike Raster Plot - {color_flashes} flashes', ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_sz = 0.005 # Bin size\n",
    "\n",
    "plot_raster_peth(peri_white, units, \"white\", bin_sz)\n",
    "plot_raster_peth(peri_black, units, \"black\", bin_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225a97d",
   "metadata": {},
   "source": [
    ":::{admonition} How to choose bin size?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "The bin size (```bin_sz```) refers to the size of the \"binning\" of the time series. A bin size of 0.005 means that one second will be split into 200 bins. Larger bins indicate lower temporal resolution, while smaller bins indicate higher temporal resolution. In this tutorial, we chose 0.005 as bin size , but it is worth noting that are different ways of calculating this value. How many bins you need depends on the temporal resolution of the process that you are modeling. This is a modelers choice, and there is no bin size fits all!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81eec89",
   "metadata": {},
   "source": [
    ":::{admonition} Why does the peth plot look so smooth?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "We are using the [`smooth`](https://pynapple.org/generated/pynapple.Tsd.smooth.html#pynapple.Tsd.smooth) function from Pynapple to apply Gaussian smoothing to the perievent time series before plotting. This reduces trial-to-trial variability and emphasizes consistent temporal patterns in firing rate, making features like peaks or latency shifts easier to interpret—especially when spike trains are noisy or sparse.\n",
    "\n",
    "In this tutorial, we use a Gaussian kernel with a standard deviation of 0.015 seconds.  \n",
    "Given our bin size of `bin_sz = 0.005`, the sampling rate is: ```1 / bin_sz = 200 Hz```\n",
    "\n",
    "This means we smooth over approximately 3 time bins.\n",
    "\n",
    "For implementation details, refer to the [Pynapple documentation](https://pynapple.org/generated/pynapple.Tsd.smooth.html#pynapple.Tsd.smooth).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340525a8",
   "metadata": {},
   "source": [
    "In the plot above, we can see that some units (951765552, blue or 951765582, purple) are clearly more responsive than others (951765571, red - or even 951765566, green), which are apparently not modulated by the flashes. Thus, it would make sense to take a subset of the neurons, the most responsive ones, and model those.\n",
    "\n",
    "We will now calculate responsiveness for each neuron as the normalized difference between average firing rate before and after stimulus presentation, and select the most responsive ones for further analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12020f6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def get_responsiveness(perievents, bin_sz):\n",
    "    \"\"\"Calculate responsiveness for each neuron. This is\n",
    "    computer as:\n",
    "\n",
    "    post_stim_av  : \n",
    "        Average firing rate during presentation (250ms) of stimulus across\n",
    "        all instances of stimulus. \n",
    "\n",
    "    pre_stim_av :\n",
    "        Average firing rate prior (250ms) to the presentation of stimulus\n",
    "        accross all instances prior of stimulus. \n",
    "\n",
    "    responsiveness : \n",
    "        (post_stim_ave - pre_stim_av) / (post_stim_ave + pre_stim_av)\n",
    "\n",
    "    Larger values indicate higher responsiveness to the stimuli.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    perievents : TsGroup\n",
    "        Contains perievent information of a subset of neurons\n",
    "    bin_sz : float\n",
    "        Bin size for calculating spike counts\n",
    "\n",
    "    Returns\n",
    "    ----------   \n",
    "    resp_array : np.array\n",
    "        Array of responsiveness information.\n",
    "    resp_dict : dict\n",
    "        Dictionary of responsiveness information. Indexed by each neuron's,\n",
    "        contains responsiveness, pre_stim_av and post_stim_av information\n",
    "\n",
    "    \"\"\"\n",
    "    resp_dict = {}\n",
    "    resp_array = np.zeros(len(perievents.keys()), dtype=float)\n",
    "\n",
    "    for index,unit in enumerate(perievents.keys()):\n",
    "        # Count the number of timestamps in each bin_sz bin.\n",
    "        peri_counts = perievents[unit].count(bin_sz)\n",
    "\n",
    "        # Get the firing rate\n",
    "        peri_rate = peri_counts/bin_sz\n",
    "\n",
    "        # Compute average firing rate for each milisecond in the\n",
    "        # the 250ms before stimulus presentation\n",
    "        pre_stim = np.mean(peri_rate,1).restrict(nap.IntervalSet([-.25,0]))\n",
    "\n",
    "        # Compute average firing rate for each milisecond in the\n",
    "        # the 250ms after stimulus presentation\n",
    "        post_stim = np.mean(peri_rate,1).restrict(nap.IntervalSet([0,.25]))\n",
    "\n",
    "        pre_stim_av = np.mean(pre_stim)\n",
    "        post_stim_av = np.mean(post_stim)\n",
    "        responsiveness = (post_stim_av - pre_stim_av) / (post_stim_av + pre_stim_av)\n",
    "\n",
    "        resp_dict[unit] = {\n",
    "            \"responsiveness\": responsiveness,\n",
    "            \"pre_stim_av\": pre_stim_av,\n",
    "            \"post_stim_av\": post_stim_av,\n",
    "        }\n",
    "        resp_array[index] = responsiveness\n",
    "\n",
    "    return resp_array, resp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate responsiveness\n",
    "responsiveness_white,_ = get_responsiveness(peri_white, bin_sz)\n",
    "responsiveness_black,_ = get_responsiveness(peri_black, bin_sz)\n",
    "\n",
    "# Add resposiveness as metadata for units\n",
    "units.set_info(responsiveness_white=responsiveness_white)\n",
    "units.set_info(responsiveness_black=responsiveness_black)\n",
    "\n",
    "# See metadata\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf001b",
   "metadata": {},
   "source": [
    "Now I can remove units without any firing throughout the whole stimulus presentation, with 0 responsiveness and also keep the top 30% most responsive units for ongoing analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f40900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nan values ocurr when there is division by zero aka firing rate before + firing rate after = 0 \n",
    "units = units[~np.isnan(units[\"responsiveness_black\"]) & ~np.isnan(units[\"responsiveness_white\"])]\n",
    "\n",
    "# Remove units with 0 responsiveness\n",
    "units = units[(np.abs(units[\"responsiveness_black\"])>0.01 ) & (np.abs(units[\"responsiveness_white\"]) >0.01)]\n",
    "\n",
    "# Get threshold for top 30% most resopnsive\n",
    "thresh_black = np.percentile(units[\"responsiveness_black\"], 70)\n",
    "thresh_white = np.percentile(units[\"responsiveness_white\"], 70)\n",
    "\n",
    "# Only keep units that are within the 30% most responsive for either black or white\n",
    "units = units[(units[\"responsiveness_black\"] > thresh_black) | (units[\"responsiveness_white\"] > thresh_white)]\n",
    "\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ff7c7",
   "metadata": {},
   "source": [
    "### Revision of stimuli and spiking\n",
    "\n",
    "Now that we have selected the units for our analyses, we can see how some of these neurons look in response to the stimuli in a PETH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a31ff",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_peri_side_by_side(params,\n",
    "                           bin_sz,\n",
    "                           simulation = False,\n",
    "                           smoothing = 0.015,\n",
    "                           ):\n",
    "    \"\"\" Top neurons should be range from 0 to last shape dim in case of simulation\n",
    "    \"\"\"\n",
    "    peri_white, peri_black = params\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)  # 1 row, 2 columns\n",
    "\n",
    "    # Plot White\n",
    "    ax = axs[0]\n",
    "    \n",
    "    for unit in list(peri_white.keys()):\n",
    "        if simulation and not hist: \n",
    "            peri_u = np.mean(peri_white,axis=1)[:,unit]\n",
    "            ax.plot(peri_u, linewidth=1)\n",
    "        else:\n",
    "            peri_u = peri_white[unit]\n",
    "            peri_u_count = peri_u.count(bin_sz)\n",
    "\n",
    "            peri_u_coun_conv_mean = np.mean(peri_u_count, 1).smooth(std=smoothing)\n",
    "            peri_u_rate_conv = peri_u_coun_conv_mean / bin_sz\n",
    "            ax.plot(peri_u_rate_conv, linewidth=1)\n",
    "    \n",
    "\n",
    "    ax.set_xlim(-0.25, 0.50) # restrict\n",
    "    ax.axvspan(0, 0.250, color=\"silver\", alpha=0.3, ec=\"black\")\n",
    "    ax.axvline(0.0, color=\"black\")\n",
    "    ax.set_ylabel(\"Firing rate (Hz)\")\n",
    "    ax.set_title(\"Perievent white flashes\")\n",
    "\n",
    "    # Plot Black\n",
    "    ax = axs[1]\n",
    "    for unit in list(peri_black.keys()):\n",
    "        if simulation: \n",
    "            peri_u = np.mean(peri_black,axis=1)[:,unit]\n",
    "            ax.plot(peri_u, linewidth=1)\n",
    "        else:\n",
    "            peri_u = peri_black[unit]\n",
    "            peri_u_count = peri_u.count(bin_sz)\n",
    "\n",
    "            peri_u_coun_conv_mean = np.mean(peri_u_count, 1).smooth(std=smoothing)\n",
    "            peri_u_rate_conv = peri_u_coun_conv_mean / bin_sz\n",
    "            ax.plot(peri_u_rate_conv, linewidth=1)\n",
    "\n",
    "    ax.set_xlim(-0.25, 0.50) # restrict\n",
    "    ax.axvspan(0, 0.250, color=\"black\", alpha=0.3, ec=\"black\")\n",
    "    ax.axvline(0.0, color=\"black\")\n",
    "    ax.set_title(\"Perievent black flashes\")\n",
    "    fig.text(0.5, -.01, 'Time from stim(s)', ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if worth to index perievents calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e985cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perievent for white stimuli - subset of units\n",
    "peri_white = nap.compute_perievent(timestamps = units,\n",
    "                                        tref = flashes_white.starts,\n",
    "                                        minmax = window_size\n",
    ")\n",
    "\n",
    "# Calculate perievent for black stimuli - subset of units\n",
    "peri_black = nap.compute_perievent(timestamps = units,\n",
    "                                        tref = flashes_black.starts,\n",
    "                                        minmax = window_size\n",
    ")\n",
    "\n",
    "params_obs = [peri_white, \n",
    "              peri_black]\n",
    "\n",
    "plot_peri_side_by_side(\n",
    "    params_obs,\n",
    "    bin_sz\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a4de3",
   "metadata": {},
   "source": [
    "or in a raster plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009de938",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def raster_plot(units, n_neurons = len(units), n_flashes = 5, n_seconds = 13, offset = .5):\n",
    "    n_neurons = len(units)\n",
    "    n_flashes = 5\n",
    "    n_seconds = 13\n",
    "    offset = .5\n",
    "\n",
    "    start = data[\"flashes_presentations\"][\"start\"].min() - offset # Start a little bit earlier than the first flash presentation\n",
    "    end = start + n_seconds\n",
    "\n",
    "    # Restrict the neurons to the period of presentation of plotted stimuli\n",
    "    units = units.restrict(nap.IntervalSet(start, end))\n",
    "\n",
    "    # Change type to tsd to access spiking times. \n",
    "    # Change the metadata information so each different \n",
    "    # neuron is now tagged with an ID from 1 to 10\n",
    "    # for ease of plot\n",
    "    neurons_to_plot = units.to_tsd([i+1 for i in range(n_neurons)])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (17, 4))\n",
    "\n",
    "    ax.plot(neurons_to_plot, \"|\", markersize=2, mew=2)\n",
    "\n",
    "    # Different coloured flashes\n",
    "    [ax.axvspan(\n",
    "        s, \n",
    "        e, \n",
    "        color = \"silver\", \n",
    "        alpha=.4, \n",
    "        ec=\"black\"\n",
    "        ) for s, e in zip(\n",
    "            flashes_white[:n_flashes].start, \n",
    "            flashes_white[:n_flashes].end\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    [ax.axvspan(\n",
    "        s, \n",
    "        e, \n",
    "        color = \"black\", \n",
    "        alpha=.4, \n",
    "        ec=\"black\"\n",
    "        ) for s, e in zip(\n",
    "            flashes_black[:n_flashes].start, \n",
    "            flashes_black[:n_flashes].end\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Unit\")\n",
    "    ax.set_title(\"Primary Visual Cortex (VISp) units spikes and stimuli\")\n",
    "    plt.xlim(start,end) \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_plot(units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c79c74",
   "metadata": {},
   "source": [
    "Importantly, we are interested solely in the response of the unit around the stimuli. Let's restrict our time series around the stimuli presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af193641",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = .50 # 500 ms\n",
    "start = flashes.start - dt # Start 500ms before stimulus presentation\n",
    "end = flashes.end + dt # End 500ms after stimulus presentation\n",
    "\n",
    "ep_flashes = nap.IntervalSet(start,end, metadata=flashes.metadata) \n",
    "print(ep_flashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc634fb",
   "metadata": {},
   "source": [
    "Create one object for white and another for black flashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_flashes_white = ep_flashes[ep_flashes[\"color\"]==\"1.0\"]\n",
    "ep_flashes_black = ep_flashes[ep_flashes[\"color\"]==\"-1.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137708f8",
   "metadata": {},
   "source": [
    "### Splitting the dataset in train and test\n",
    "We could train the model with the whole dataset, but then we would not have a way to assess whether our model is capable of making accurate predictions or simply overfitting to our data. The simplest way around this is to have a reserved part of the data for testing. However, the precise details on how to do this splitting depends on the question, the data, and the researcher.\n",
    "\n",
    "Here, we will split the data in two: 70% will be for training and 30% will be for testing. However, we can't simply grab the first 70% of the timeseries - what if we are biasing our sample and there are some neurons that respond only towards the end or the beginning of the experiment? For that, we will gather one every three flashes, and those will go to the testing set. The rest, will go to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take one every three flashes (33% of all flashes of test)\n",
    "flashes_test_white = ep_flashes_white[::3]\n",
    "flashes_test_black = ep_flashes_black[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac3f19",
   "metadata": {},
   "source": [
    "Pynapple has a nice function to get all the epochs: [```set_diff```](https://pynapple.org/generated/pynapple.IntervalSet.set_diff.html). With it, we can get all of the interval sets which are not in the interval set passed as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The remaining is separated for training\n",
    "flashes_train_white = ep_flashes_white.set_diff(flashes_test_white)\n",
    "flashes_train_black = ep_flashes_black.set_diff(flashes_test_black)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121f055",
   "metadata": {},
   "source": [
    "Consider black and white for test and train\n",
    "using [```union```](https://pynapple.org/generated/pynapple.IntervalSet.union.html#pynapple.IntervalSet.union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9461b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge both stimuli types in a single interval set\n",
    "flashes_test = flashes_test_white.union(flashes_test_black)\n",
    "flashes_train = flashes_train_white.union(flashes_train_black)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63893c4",
   "metadata": {},
   "source": [
    "Now that we have our intervals correctly, we can use [```restrict```](https://pynapple.org/generated/pynapple.TsGroup.restrict.html#pynapple.TsGroup.restrict) to get our test and train sets for units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General spike counts\n",
    "units_counts = units.count(bin_sz, ep = ep_flashes)\n",
    "\n",
    "# Restrict counts to test and train\n",
    "units_counts_test = units_counts.restrict(flashes_test)\n",
    "units_counts_train = units_counts.restrict(flashes_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c40db",
   "metadata": {},
   "source": [
    "## Fitting a GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b78ff",
   "metadata": {},
   "source": [
    "### Preparing the data for NeMos\n",
    "Now that we have a good understanding of our data, and that we have splitted our dataset in the corresponding test and train subsets, we are almost ready to run our model. However, before we can construct it, we need to get our data in the right format.\n",
    "\n",
    "When fitting a single neuron, NeMoS requires that the predictors and spike counts it operates on have the following properties:\n",
    "\n",
    "- predictors and spike counts must have the same number of time points.\n",
    "- predictors must be two-dimensional, with shape ```(n_time_bins, n_features)```. So far, we have two features in this tutorial: white and black flashes.\n",
    "- spike counts must be one-dimensional, with shape ```(n_time_bins,)```. \n",
    "- predictors and spike counts must be jax.numpy arrays, numpy arrays or pynapple TsdFrame/Tsd.\n",
    "\n",
    "When fitting multiple neurons, spike counts must be two-dimensional: ```(n_time_bins, n_neurons)```. Moreover, in that case, spike counts can be Pynapple TsGroup objects as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc22a3",
   "metadata": {},
   "source": [
    "First, we require that our predictors and our spike counts have the same number of time bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4039c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TsdFrame filled by zeros, for the size of units_counts\n",
    "predictors = nap.TsdFrame(t=units_counts.t, d=np.zeros((len(units_counts), 2)), columns = ['white', 'black'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758a727",
   "metadata": {},
   "source": [
    "At the moment, the flashes are in a IntervalSet, we need to grab them and make them time series of stimuli, separated by black and white (because we are interested in how neurons' responses are modulated by each individual stimulus type separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether there is a flash within a given bin of spikes\n",
    "# If there is not, put a nan in that index\n",
    "idx_white = flashes_white.in_interval(units_counts)\n",
    "idx_black = flashes_black.in_interval(units_counts)\n",
    "\n",
    "# Replace everything that is not nan with 1 in the corresponding column\n",
    "predictors.d[~np.isnan(idx_white), 0] = 1\n",
    "predictors.d[~np.isnan(idx_black), 1] = 1\n",
    "\n",
    "print(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"predictors shape: {predictors.shape}\")\n",
    "print(f\"\\ncount shape: {units_counts.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f52ed",
   "metadata": {},
   "source": [
    "They match in the first dimension because they have the same number of timepoints! Meanwhile, in the second dimension, counts has 47 because the selected units for this tutorial sums to 47, while predictors is 2 because we have black and white flashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417abae6",
   "metadata": {},
   "source": [
    "Just to make sure that we got the right output, let's plot our new \"predictors\" time series as lines alongside our first plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb00dd",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def stimuli_plot(predictors, n_flashes = 5, n_seconds = 13, offset = .5):\n",
    "    n_flashes = 5\n",
    "    n_seconds = 13\n",
    "    offset = .5\n",
    "\n",
    "    start = data[\"flashes_presentations\"][\"start\"].min() - offset # Start a little bit earlier than the first flash presentation\n",
    "    end = start + n_seconds\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (17, 4))\n",
    "\n",
    "    #ax.plot(neurons_to_plot, \"|\", markersize=2, mew=2)\n",
    "\n",
    "    # Different coloured flashes\n",
    "    [ax.axvspan(s, e, color = \"silver\", alpha=.4, ec=\"black\") for s, e in zip(flashes_white[:n_flashes].start, flashes_white[:n_flashes].end)]\n",
    "    [ax.axvspan(s, e, color = \"black\", alpha=.4, ec=\"black\") for s, e in zip(flashes_black[:n_flashes].start, flashes_black[:n_flashes].end)]\n",
    "    plt.plot(predictors[\"white\"], \"o-\", color= \"silver\")\n",
    "    plt.plot(predictors[\"black\"], \"o-\", color= \"black\")\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Absent = 0, Present = 1\")\n",
    "    ax.set_title(\"Presented Stimuli\")\n",
    "    \n",
    "    plt.xlim(start,end) # Note to Guillaume: cant remove xlim because if i do the plot looks weird at the beginning as if there was no spiking\n",
    "\n",
    "    # Only use integer values for ticks\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88025c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_plot(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e1ade",
   "metadata": {},
   "source": [
    "They match perfectly! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61412a0",
   "metadata": {},
   "source": [
    "Secondly, we need to ensure our variables are the proper shape:\n",
    "\n",
    "- ```predictors : (n_time_bins, n_features)```\n",
    "- ```counts : (n_time_bins, n_neurons)```\n",
    "\n",
    "In principle, our current variables are in a proper shape - but in a moment we will see why this set up of predictors is not ideal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd4c7d",
   "metadata": {},
   "source": [
    "As a last preprocessing step, let's just split these predictors in train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4246a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_test = predictors.restrict(flashes_test)\n",
    "predictors_train = predictors.restrict(flashes_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da3c24",
   "metadata": {},
   "source": [
    "### Constructing the design matrix using basis functions\n",
    "\n",
    "Right now, our predictors are composed by black and white flashes at every time point, but that would imply assuming that the neuron's spiking behavior is only driven by the instantaneous flash presentation. However, we know neurons integrate information over time, so why not changing our predictors to reflect that? \n",
    "\n",
    "We can achieve this by including variables that represent the history of exposure to the flashes. For this, we must decide the duration of time that we think is relevant: does the exposure to flashes 10ms ago matter? What about 100ms ago? 1s? We should use priori knowledge of our system to determine a initial value. \n",
    "\n",
    "For this tutorial, we will use the whole duration of the stimuli as relevant history. That is, we will model each unit’s response to 250 ms full-field flashes by capturing how stimulus history over that duration influences spiking. We therefore define a 250 ms stimulus window, aligned with the flash onset, which spans the entire stimulus duration. This window enables the GLM to learn how the neuron's firing rate evolves throughout the flash. Using a shorter window could omit delayed effects, while a longer window may incorporate unrelated post-stimulus activity.\n",
    "\n",
    "To construct our stimulus history predictor, we could generate time-lagged copies of the stimulus input (in the form of a [Hankel Matrix](https://en.wikipedia.org/wiki/Hankel_matrix)). Specifically, the value of the first predictor at time $ t $ would correspond to the stimulus at time $ t $, while the second predictor would capture the stimulus at time $ t - 1 $ , and so on, up to a maximum lag corresponding to the length of the stimulus integration window (250 ms in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d078fb7",
   "metadata": {},
   "source": [
    ":::{admonition} How do you build a Hankel matrix?\n",
    ":class: info\n",
    "\n",
    "Every row is a shifted copy of the row above!\n",
    "\n",
    "![henkel_matrix](../../data/images/henkel_matrix.gif)\n",
    "\n",
    "Construction of Hankel Matrix. Modified from <a href=\"https://pillowlab.princeton.edu/pubs/pillow_TutorialSlides_Cosyne2018.pdf\">Pillow 2018 Cosyne tutorial </a>\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a1f3d",
   "metadata": {},
   "source": [
    "However, modeling each time lag with an independent parameter leads to a high-dimensional filter that is prone to overfitting! (Given that we are using a bin size of 0.005, we would end up with 50 lags = 50 parameters per flash color!) A better idea is to do some dimensionality reduction on these predictors, by parametrizing them using basis functions. These will allow us to capture interesting non-linear effects with a relatively low-dimensional parametrization that preserves convexity. \n",
    "\n",
    "NeMoS has a whole library of basis objects available at [```nmo.basis```](https://nemos.readthedocs.io/en/latest/background/basis/README.html#table-basis), and choosing which set of basis functions and their parameters, like choosing the duration of the flash history predictor, requires knowledge of your problem, but can later be examined using model comparison tools.\n",
    "\n",
    "For history-type inputs like we’re discussing, the raised cosine log-stretched basis first described in [Pillow et al., 2005](https://www.jneurosci.org/content/25/47/11003) is a good fit. This basis set has the nice property that their precision drops linearly with distance from event, which is a makes sense for many history-related inputs in neuroscience: whether an input happened 1 or 5 ms ago matters a lot, whereas whether an input happened 51 or 55 ms ago is less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8033204b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Duration of stimuli\n",
    "stimulus_history_duration = 0.25 \n",
    "\n",
    "# Window length in bin size units\n",
    "window_len = int(stimulus_history_duration / bin_sz)\n",
    "\n",
    "basis_example = nmo.basis.RaisedCosineLogConv(\n",
    "    n_basis_funcs = 5, \n",
    "    window_size = window_len, \n",
    ")\n",
    "sample_points, basis_values = basis_example.evaluate_on_grid(100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.plot(sample_points, basis_values)\n",
    "ax.set_title(\"Raised cosine log-stretched basis\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "ax.set_xlabel(\"Time Lag\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b1882",
   "metadata": {},
   "source": [
    "When we instantiate this object, the only arguments we need to specify is the number of functions we want and the mode of operation of the basis:\n",
    "- Number of functions: with more basis functions, we’ll be able to represent the effect of the corresponding input with the higher precision, at the cost of adding additional parameters.\n",
    "- Mode of operation: either ```Conv``` for convolutional or ```Eval```for evaluation form of the basis. This is determined by the type of feature we aim to represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8aee9",
   "metadata": {},
   "source": [
    ":::{admonition} When should I use the convolutional or evaluation form of the basis?\n",
    ":class: info\n",
    "- Evaluation bases transform the input through the non-linear function defined by the basis. This can be used to represent features such as spatial location and head direction.\n",
    "- Convolution bases apply a convolution of the input data to the bank of filters defined by the basis, and is particularly useful when analyzing data with inherent temporal dependencies, such as spike history or the history of flash exposure in this example. In convolution mode, we must additionally specify the ```window_size```, the length of the filters in bins.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration of stimuli\n",
    "stimulus_history_duration = 0.25 \n",
    "\n",
    "# Window length in bin size units\n",
    "window_len = int(stimulus_history_duration / bin_sz)\n",
    "\n",
    "# Initialize basis objects\n",
    "# White basis\n",
    "basis_white = nmo.basis.RaisedCosineLogConv(\n",
    "    n_basis_funcs = 5, \n",
    "    window_size = window_len, \n",
    "    label = \"white\"\n",
    ")\n",
    "# Black basis\n",
    "basis_black = nmo.basis.RaisedCosineLogConv(\n",
    "    n_basis_funcs = 5, \n",
    "    window_size = window_len, \n",
    "    label = \"black\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27ca52",
   "metadata": {},
   "source": [
    "Using the ```compute_features```function, NeMos convolves our input features (predictors) with the basis object to compress them. How does that look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60b04a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Use only first flash presentation for plot\n",
    "interval= flashes_train_white[0]\n",
    "\n",
    "# Compute features\n",
    "features = basis_white.compute_features(predictors_train[\"white\"])\n",
    "\n",
    "# Initialize plot\n",
    "fig, axes = plt.subplots(2, 3, sharey=\"row\", figsize=(7.5, 3.5), tight_layout=True)\n",
    "\n",
    "# Create time axis\n",
    "time, basis = basis_white.evaluate_on_grid(basis_white.window_size)\n",
    "time *= window_len\n",
    "\n",
    "# Restrict predictors and features to first flash presentation\n",
    "flashes = predictors_train[\"white\"].restrict(interval)\n",
    "features = features.restrict(interval) \n",
    "\n",
    "# Plot raw predictors\n",
    "for ax in axes[1, :]:\n",
    "    ax.plot(flashes, \"k--\", label=\"true\")\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "\n",
    "# Plot first basis function\n",
    "axes[0, 0].plot(time, basis, alpha=0.1)\n",
    "axes[0, 0].set_xticks([])\n",
    "axes[0, 0].plot(time, basis[:, 0], \"C0\", alpha=1)\n",
    "axes[0, 0].set_ylabel(\"Amp.\")\n",
    "axes[0, 0].set_yticks([])\n",
    "\n",
    "# Plot first Feature (first basis function convolved with input)\n",
    "axes[0, 0].set_title(\"Feature 1\")\n",
    "axes[1, 0].plot(features[:, 0], label=\"conv.\")\n",
    "axes[1, 0].set_ylabel(\"Flash\")\n",
    "axes[1, 0].set_yticks([])\n",
    "\n",
    "# Plot last basis function\n",
    "axes[0, 1].plot(time, basis[:, -1], f\"C{basis.shape[1]-1}\", alpha=1)\n",
    "axes[0, 1].set_xticks([])\n",
    "axes[0, 1].set_title(f\"Feature {basis.shape[1]}\")\n",
    "\n",
    "# Plot last feature\n",
    "axes[1, 1].plot(features[:, -1], f\"C{basis.shape[1]-1}\")\n",
    "axes[0, 1].plot(time, basis, alpha=0.1)\n",
    "\n",
    "# Plot all basis functions\n",
    "axes[0, 2].plot(time, basis)\n",
    "axes[0, 2].set_xticks([])\n",
    "axes[0, 2].set_title(\"All features\")\n",
    "\n",
    "# Plot all features\n",
    "axes[1, 2].plot(features)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3fca23",
   "metadata": {},
   "source": [
    "On the top row, we can see the basis function, same as in the plot above. On the bottom row, we are showing one flash presentation, as a dashed line, and corresponding features over a small window of time. These features are the result of a convolution between the basis function on the top row with the black dashed line showed below. The basis functions get progressively wider and delayed from the flash onset, so we can think of the features as weighted averages that get progressively later and smoother.\n",
    "\n",
    "In the leftmost plot, we can see that the first feature almost perfectly tracks the input. Looking at the basis function above, that makes sense: this function’s max is at 0 and quickly decays. In the middle plot, we can see that the last feature has a fairly long lag compared to the flash, and is a lot smoother. Looking at the rightmost plot, we can see that the other features vary between these two extremes, getting smoother and more delayed.\n",
    "\n",
    "These are the elements of our feature matrix: representations of not just the instantaneous presentation of a flash, but also the its history. Let’s see what this looks like when we go to fit the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90350ff0",
   "metadata": {},
   "source": [
    "In our case, we want our basis to be composed by both black and white flashes features. For that, we can build an additive basis, simply by adding our already declared basis objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4068c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additive basis object\n",
    "additive_basis = basis_white + basis_black\n",
    "\n",
    "# Convolve basis with inputs - train set\n",
    "X_train = additive_basis.compute_features(\n",
    "    predictors_train[\"white\"], # Corresponding to basis_white\n",
    "    predictors_train[\"black\"]  # Corresponding to basis_black\n",
    ")\n",
    "\n",
    "# Convolve basis with inputs - test set\n",
    "X_test = additive_basis.compute_features(\n",
    "    predictors_test[\"white\"], \n",
    "    predictors_test[\"black\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e546209",
   "metadata": {},
   "source": [
    ":::{admonition} More resources on basis functions\n",
    ":class: info\n",
    ":class: dropdown\n",
    "- [Nemos fit head-direction population tutorial](https://nemos.readthedocs.io/en/latest/tutorials/plot_02_head_direction.html): For a step by step explanation of how to build the design matrix first as a result of convolving the features with the identity matrix, and then by using basis functions, alongside nice visualizations.\n",
    "- [Flatiron Institute Introduction to GLMs tutorial](https://flatironinstitute.github.io/neurorse-workshops/workshops/jan-2025/branch/main/full/day2/current_injection.html#fitting-the-model): For a detailed explanation, step by step, on how predictors look with and without basis functions, with nice visualizations as well.\n",
    "- [Bishop, 2009](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf): Section 3.1 for a formal description of what basis functions are and some examples of them.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d784d",
   "metadata": {},
   "source": [
    "### Initialize and fit a GLM: single unit\n",
    "\n",
    "Now we are finally ready to start our model!\n",
    "\n",
    "First, we need to define our GLM model object. To initialize our model, we need to specify the ```solver_name```, the ```regularizer```, the ```regularizer_strength``` and the ```observation_model```. All of these are optional.\n",
    "\n",
    "- ```solver_name```: this string specifies the solver algorithm. The default behavior depends on the regularizer, as each regularization scheme is only compatible with a subset of possible solvers.\n",
    "- ```regularizer```: this string or object specifies the regularization scheme. Regularization modifies the objective function to reflect your prior beliefs about the parameters, such as sparsity. Regularization becomes more important as the number of input features, and thus model parameters, grows. NeMoS’s solvers can be found within the nemos.regularizer module. If you pass a string matching the name of one of our solvers, we initialize the solver with the default arguments. If you need more control, you will need to initialize and pass the object yourself.\n",
    "- ```observation_model```: this object links the firing rate and the observed data (in this case spikes), describing the distribution of neural activity (and thus changing the log-likelihood). For spiking data, we use the Poisson observation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e5e5a",
   "metadata": {},
   "source": [
    "For this tutorial, we’ll use a LBFGS solver with Ridge regularization, and a regularization strength of 0.005."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039437b3",
   "metadata": {},
   "source": [
    ":::{admonition} Why LBFGS?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "LBFGS is a quasi-Netwon method, that is, it uses the first derivative (the gradient) and approximates the second derivative (the Hessian) in order to solve the problem. This means that LBFGS tends to find a solution faster and is often less sensitive to step-size. Try other solvers to see how they behave!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd7bdb",
   "metadata": {},
   "source": [
    ":::{admonition} What is regularization?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "When fitting models, it is generally recommended to use regularization, a technique that adds a constraint or penalty to the model’s cost function. Regularization works by discouraging the coefficients from reaching large values.\n",
    "\n",
    "Penalizing large coefficients is beneficial because it helps prevent overfitting, a phenomenon in which the model fits the training data too closely, capturing noise instead of the underlying pattern. Large coefficients often indicate a model that is too complex or sensitive to small fluctuations in the data. By keeping coefficients smaller and more stable, regularization promotes simpler models that generalize better to unseen data, improving predictive performance and robustness.\n",
    "\n",
    "In this tutorial, we will use Ridge regularization (or L2 regularization). In this type of regularization, the penalty term added to the loss function is:\n",
    "\n",
    "$$\n",
    "\\text{Penalty} = \\frac{\\lambda}{2N} \\sum_{j} \\theta_j^2\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization strength, $N$ is the number of samples and $\\theta_j$ are the model coefficients, stored in model.coef_\n",
    "\n",
    "Please refer to [NeMos documentation](https://nemos.readthedocs.io/en/latest/generated/regularizer/nemos.regularizer.Ridge.html#nemos.regularizer.Ridge) for more details on how this was implemented.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer_strength = 0.0004\n",
    "# Initialize model object of a single unit\n",
    "model = nmo.glm.GLM(\n",
    "    regularizer = \"Ridge\",\n",
    "    regularizer_strength = regularizer_strength,\n",
    "    solver_name=\"LBFGS\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1b4f4",
   "metadata": {},
   "source": [
    ":::{admonition} Where did the regularization strength value come from?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "We conducted cross validation to obtain the regularization strength:\n",
    "\n",
    "```\n",
    "# Initialize model object\n",
    "model = nmo.glm.GLM(\n",
    "    regularizer = \"Ridge\",\n",
    "    regularizer_strength = 0.01,\n",
    "    solver_name=\"LBFGS\", \n",
    "    #solver_kwargs=dict(tol=10**-12)\n",
    ")\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = {\n",
    "    \"regularizer_strength\" : \n",
    "    np.geomspace(10**-9, 10, 10)\n",
    "}\n",
    "\n",
    "# Instantiate the grid search object\n",
    "grid_search = GridSearchCV(\n",
    "    model,param_grid, \n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "# Run grid search\n",
    "grid_search.fit(X_train, u_counts_train)\n",
    "\n",
    "# Print optimal parameter\n",
    "print(grid_search.best_estimator_.regularizer_strength)\n",
    ">>> 0.00035938136638046257\n",
    "```\n",
    "::: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15422b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an example unit\n",
    "unit_id = 951765530\n",
    "\n",
    "# Get counts for train and test for said unit\n",
    "u_counts_train = units_counts_train.loc[unit_id]\n",
    "u_counts_test = units_counts_test.loc[unit_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea28862",
   "metadata": {},
   "source": [
    "We intend the NeMos models to be used like [scikit-learn](https://scikit-learn.org/stable/getting_started.html) estimators. In these, a model instance is initialized with hyperparameters (like regularization strength, solver, etc), and then we can call the ```fit()``` function to fit the model to data. Since we have already created our model and have our data, we can go ahead and call ```fit()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, u_counts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd00c0a",
   "metadata": {},
   "source": [
    "Now that we’ve fit our data, we can retrieve the resulting parameters. Similar to scikit-learn, these are stored as the ```coef_```and ```intercept_``` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"firing_rate(t) = exp({model.coef_} * flash(t) + {model.intercept_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ead33",
   "metadata": {},
   "source": [
    "Note that ```model.coef_``` has shape ```(n_features, )```, while ```model.intercept_``` has shape (```n_neurons```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_.shape)\n",
    "print(model.intercept_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ff692",
   "metadata": {},
   "source": [
    "### Assess GLM performance: predict\n",
    "\n",
    "Although it is nice to see the parameters, just by looking at them we cannot tell how good our model is doing - How do we assess this?\n",
    "\n",
    "For this, we can use the model to predict firing rates and compare that to the smoothed spike train. By calling [```predict```](https://nemos.readthedocs.io/en/latest/generated/glm/nemos.glm.GLM.predict.html#nemos.glm.GLM.predict), we can get the model's predicted firing rate for this data (the output of the nonlinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93152fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use predict to obtain the firing rates\n",
    "pred_unit = model.predict(X_test)\n",
    "\n",
    "# convert units from spikes/bin to spikes/sec\n",
    "pred_unit = pred_unit/ bin_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9054b139",
   "metadata": {},
   "source": [
    "Now, we can use pynapple function ```perievent_continuous``` to re-center the timestamps of the predicted rates around the stimulus presentations, in a similar manner than at the beginning of the tutorial. In contrast to ```perievent```, ```perievent_continuous``` allows us to center a continuous time series, which is precisely what we have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"pred_unit type: {type(pred_unit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db7e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = (-.250,.500)\n",
    "peri_white_pred_unit = nap.compute_perievent_continuous(timeseries = pred_unit, \n",
    "                                        tref = nap.Ts(flashes_test_white.start+.50), # +50 because we added +.50 at beginning and end of stimulus\n",
    "                                        minmax=window_size\n",
    ")  \n",
    "peri_black_pred_unit = nap.compute_perievent_continuous(timeseries = pred_unit, \n",
    "                                        tref = nap.Ts(flashes_test_black.start+.50), # +50 because we added +.50 at beginning and end of stimulus\n",
    "                                        minmax=window_size\n",
    ")  \n",
    "\n",
    "peri_white_pred_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234d4b1",
   "metadata": {},
   "source": [
    "The resulting object is a Pynapple ```TsdFrame``` of shape time x trials (we are defining one trial as one presentation of stimuli).\n",
    "\n",
    "Now, we can plot the PETH of both the average rate of unit and the average predicted rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d87e0e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_peri_predict(\n",
    "        peri_white_pred_unit, \n",
    "        peri_black_pred_unit, \n",
    "        peri_white, \n",
    "        peri_black,\n",
    "        unit_id = unit_id\n",
    "    \n",
    "):\n",
    "    fig, ax = plt.subplots(1,2,figsize=(17, 4), sharey=True)\n",
    "    color = (0.5803921568627451, 0.403921568627451, 0.7411764705882353)\n",
    "    ### white\n",
    "    # predicted\n",
    "    ax[0].plot(np.mean(peri_white_pred_unit,axis=1), linewidth=2, color=\"red\", label = \"predicted\")\n",
    "\n",
    "    peri_u = peri_white[unit_id]\n",
    "    peri_u_count = peri_u.count(bin_sz)\n",
    "\n",
    "    peri_u_count_conv_mean = np.mean(peri_u_count, 1).smooth(std=0.015)\n",
    "    peri_u_rate_conv = peri_u_count_conv_mean / bin_sz\n",
    "    # observed\n",
    "    ax[0].plot(peri_u_rate_conv, linewidth=2, color=color)\n",
    "    ax[0].axvline(0.0)\n",
    "    ax[0].axvspan(0, 0.250, color=\"silver\", alpha=0.3, ec=\"black\")\n",
    "    ax[0].set_xlim(-.25, .5)\n",
    "\n",
    "    ax[0].set_title(\"White flashes\")\n",
    "\n",
    "    #### black\n",
    "    # predicted\n",
    "    ax[1].plot(np.mean(peri_black_pred_unit,axis=1), linewidth=2, color=\"red\")\n",
    "\n",
    "    peri_u = peri_black[unit_id]\n",
    "    peri_u_count = peri_u.count(bin_sz)\n",
    "\n",
    "    peri_u_count_conv_mean = np.mean(peri_u_count, 1).smooth(std=0.015)\n",
    "    peri_u_rate_conv = peri_u_count_conv_mean / bin_sz\n",
    "    # observed\n",
    "    ax[1].plot(peri_u_rate_conv, linewidth=2, color=color, label = \"observed\")\n",
    "    ax[1].axvline(0.0)\n",
    "    ax[1].axvspan(0, 0.250, color=\"black\", alpha=0.3, ec=\"black\")\n",
    "    ax[1].set_xlim(-.25, .5)\n",
    "\n",
    "    ax[1].set_title(\"Black flashes\")\n",
    "    ax[0].set_ylabel(\"Rate (Hz)\")\n",
    "\n",
    "    fig.text(0.5, -.05, 'Time from stim(s)', ha='center')\n",
    "    fig.text(0.5, .95, f'PETH unit {unit_id}', ha='center')\n",
    "    fig.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_peri_predict(peri_white_pred_unit, \n",
    "        peri_black_pred_unit, \n",
    "        peri_white, \n",
    "        peri_black\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d48b911",
   "metadata": {},
   "source": [
    "What do we see here?\n",
    "- Predicted firing rate increases upon exposure to flashes in a similar manner as the unit — Success!\n",
    "- ?\n",
    "- ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130598de",
   "metadata": {},
   "source": [
    "In this tutorial, for conciseness, we will use the regularizer strength obtained for this single neuron for the whole population. However, please note that when running your own analysis, it is necessary to find the optimal regularizer strength per neuron, as nothing guarantees that the optimal solution for this neuron will be the optimal solution for the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ebcac",
   "metadata": {},
   "source": [
    "### Initialize and fit a GLM: population of units\n",
    "NeMoS has a separate [```PopulationGLM```](https://nemos.readthedocs.io/en/latest/how_to_guide/plot_03_population_glm.html) object for fitting a population of neurons. This is equivalent to fitting each individually in a loop, but faster. It operates very similarly to the GLM object we use here: it still expects a 2d input, with neurons concatenated along the second dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b64661",
   "metadata": {},
   "source": [
    "The first step is initializing the model, as with the ```GLM``` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stimuli = nmo.glm.PopulationGLM(\n",
    "    regularizer = \"Ridge\",\n",
    "    regularizer_strength = regularizer_strength,\n",
    "    solver_name=\"LBFGS\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84421ed1",
   "metadata": {},
   "source": [
    "Our input for the ```PopulationGLM``` can be the same basis object we used for fitting a single unit. Since we now want to fit all neurons, the counts for our model will be ```units_counts_train```. With that, we call ```model.fit()``` to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04420781",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stimuli.fit(\n",
    "    X_train,\n",
    "    units_counts_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea10c1",
   "metadata": {},
   "source": [
    "### Assess GLM Population performance: heatmap\n",
    "\n",
    "To evaluate how well our Population GLM model captures the neural responses, we can visualize the activity of all units using a heatmap. Here's how we construct it:\n",
    "\n",
    "1. Predict: we get the predicted firing rate of each timepoint for each neuron using ```predict``` with our ```PopulationGLM``` object.\n",
    "2. Re center timestamps: we can use pynapple function ```perievent_continuous``` to re-center spiking activity timestamps around the presentation of stimuli.\n",
    "2. Z-scoring: We normalize the activity of each unit by converting it to z-scores. This removes differences in firing rate scale and allows us to focus on the relative response patterns across neurons.\n",
    "3. Sorting by peak time: We then sort neurons by the time at which they show their peak response in the observed data. This reveals any sequential or structured dynamics in the population response, such as wave-like activations in response to the stimulus. We sort the observed data, and then use that sorting to order the prediction.\n",
    "4. Side-by-side comparison: Finally, we plot the observed and predicted population responses side by side. If the model captures the key features of the response, the predicted plot should resemble the observed one: we would expect to see a similar diagonal or curved band of activity, reflecting the ordered peak responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac725fc",
   "metadata": {},
   "source": [
    "Step 1: Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict spikes rate of all neurons in the population\n",
    "predicted = model_stimuli.predict(X_test)\n",
    "\n",
    "# Convert units from spikes/bin to spikes/sec\n",
    "predicted = predicted/ bin_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf949399",
   "metadata": {},
   "source": [
    ":::{admonition} Note\n",
    ":class: info\n",
    "\n",
    "Our output of predict has a different number of timepoints than the fitting counts because, for predict, we are using the test set while for fitting we are using the train set. \n",
    "\n",
    "```\n",
    "# Print shape of units_counts_train - n_timepoints x n_units\n",
    "print(units_counts_train.shape)\n",
    ">>>(25000, 47) \n",
    "\n",
    "# Print shape of predictions - n_timepoints x n_units\n",
    "print(predicted.shape)\n",
    ">>> (12500, 47)\n",
    "```\n",
    "The train set is composed of 25000 timepoints, while the test set is composed by 12500 timepoints\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15546a",
   "metadata": {},
   "source": [
    "Step 2: re-center timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c68927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perievent for test set\n",
    "peri_white_test = nap.compute_perievent_continuous(\n",
    "    timeseries = units.restrict(flashes_test).count(bin_sz),\n",
    "    tref = nap.Ts(flashes_test_white.start+.50),\n",
    "    minmax = (window_size)\n",
    ")\n",
    "\n",
    "# Calculate perievent for test set\n",
    "peri_black_test = nap.compute_perievent_continuous(\n",
    "    timeseries =  units.restrict(flashes_test).count(bin_sz),\n",
    "    tref = nap.Ts(flashes_test_black.start+.50),\n",
    "    minmax = (window_size)\n",
    ")\n",
    "\n",
    "# Calculate perievent for predicted\n",
    "peri_white_pred = nap.compute_perievent_continuous(\n",
    "    timeseries = predicted, \n",
    "    tref = nap.Ts(flashes_test_white.start+.50),\n",
    "    minmax=(window_size)\n",
    ")  \n",
    "\n",
    "# Calculate perievent for predicted\n",
    "peri_black_pred =  nap.compute_perievent_continuous(\n",
    "    timeseries = predicted, \n",
    "    tref = nap.Ts(flashes_test_black.start+.50),\n",
    "    minmax=(window_size)\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147f4a1",
   "metadata": {},
   "source": [
    "Steps 3 and 4: Z-scoring and sorting according to peak time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3401b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zscore_dic(\n",
    "        peri_white_test, \n",
    "        peri_black_test,\n",
    "        peri_white_pred,\n",
    "        peri_black_pred,\n",
    "        smoothing = 0.015):\n",
    "    \"\"\"\n",
    "    Computes z-scored, time-aligned population responses for both observed \n",
    "    and predicted data and stores the outputs in separate dictionaries\n",
    "    \n",
    "    For each stimulus condition (white, black), the function:\n",
    "    - Averages peri-stimulus time series across trials\n",
    "    - Restricts to a fixed time window around stimulus onset\n",
    "    - Applies z-scoring across time for each neuron\n",
    "    - Sorts neurons by time of peak response (in observed data)\n",
    "    - Returns sorted z-scored matrices for both  and predicted data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    peri_white_test : TsdFrame\n",
    "        Observed responses to white stimuli (trials × time × neurons).\n",
    "    peri_black_test : TsdFrame\n",
    "        Observed responses to black stimuli.\n",
    "    peri_white_pred : TsdFrame\n",
    "        Predicted responses to white stimuli.\n",
    "    peri_black_pred : TsdFrame\n",
    "        Predicted responses to black stimuli.\n",
    "    smoothing : float\n",
    "        Standard deviation for Gaussian smoothing of the perievent traces.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dic_test : dict\n",
    "        Dictionary containing:\n",
    "            - 'z': z-scored and sorted observed activity (time × neurons)\n",
    "            - 'order': neuron sorting indices based on peak response\n",
    "    dic_pred : dict\n",
    "        Dictionary containing:\n",
    "            - 'z': z-scored predicted activity, sorted using test order\n",
    "    \"\"\"\n",
    "\n",
    "    # Time window around the stimulus (250 ms before and after)\n",
    "    restriction = [-.24, .25]\n",
    "\n",
    "    # Initialize dictionaries to store processed data\n",
    "    dic_test = {\n",
    "        \"white\": {\"z\": None, \"order\": None},  # Z-scored + sorted activity + sort order\n",
    "        \"black\": {\"z\": None, \"order\": None}\n",
    "    }\n",
    "    dic_pred = {\n",
    "        \"white\": {\"z\": None},  # Z-scored + sorted predicted activity\n",
    "        \"black\": {\"z\": None}\n",
    "    }\n",
    "\n",
    "    # Process TEST data for each stimulus type\n",
    "    for color, peri in zip([\"white\", \"black\"], [peri_white_test, peri_black_test]):\n",
    "        # Restrict time window and average across trials\n",
    "        mean_peri = np.mean(\n",
    "            peri.restrict(nap.IntervalSet(restriction)), axis=1\n",
    "        ).smooth(std=smoothing)\n",
    "        # Z-score across time for each neuron (independently)\n",
    "        z_mean_peri = zscore(mean_peri, axis=0)\n",
    "        # Sort neurons by time of their peak response\n",
    "        order = np.argsort(np.argmax(z_mean_peri, axis=0))\n",
    "        # Apply sorting to z-scored data\n",
    "        z_sorted = z_mean_peri[:, order]\n",
    "        # Store results in dictionary\n",
    "        dic_test[color][\"z\"] = z_sorted\n",
    "        dic_test[color][\"order\"] = order\n",
    "\n",
    "    # Process PREDICTED data\n",
    "    for color, peri in zip(\n",
    "        [\"white\", \"black\"], \n",
    "        [peri_white_pred, peri_black_pred]):  \n",
    "        # Restrict time window and average across trials\n",
    "        mean_peri = np.mean(\n",
    "            peri.restrict(nap.IntervalSet(restriction)), axis=1\n",
    "        )\n",
    "        # Z-score across time for each neuron\n",
    "        z_mean_peri = zscore(mean_peri, axis=0)\n",
    "        # Use the same neuron ordering as in test data for comparison\n",
    "        order = dic_test[color][\"order\"]\n",
    "        # Sort predicted responses using test-data order\n",
    "        z_sorted = z_mean_peri[:, order]\n",
    "        # Store in dictionary\n",
    "        dic_pred[color][\"z\"] = z_sorted\n",
    "\n",
    "    return dic_test, dic_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain dictionaries of z scores\n",
    "dic_test, dic_pred = create_zscore_dic(\n",
    "    peri_white_test, \n",
    "    peri_black_test,\n",
    "    peri_white_pred,\n",
    "    peri_black_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dic_test[\"white\"][\"z\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2c883",
   "metadata": {},
   "source": [
    "Step 5: Plot side by side comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0552c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zscores(dic_test, dic_pred):\n",
    "    \"\"\"\n",
    "    Plot heatmaps of z-scored neuronal responses for both observed and predicted data.\n",
    "\n",
    "    For each stimulus type (white and black), the function:\n",
    "    - Plots a heatmap of z-scored activity for each unit, sorted by time of peak response\n",
    "    - Compares observed and predicted activity side-by-side\n",
    "    - Adds time markers at stimulus onset and offset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dic_test : dict\n",
    "        Dictionary with observed z-scored and sorted activity for 'white' and 'black' stimuli.\n",
    "    dic_pred : dict\n",
    "        Dictionary with predicted z-scored activity for 'white' and 'black' stimuli,\n",
    "        sorted using the same order as dic_test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays matplotlib figures.\n",
    "    \"\"\"\n",
    "    for color in [\"white\", \"black\"]:\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        fig.tight_layout()\n",
    "        fig.set_figheight(4)\n",
    "        fig.set_figwidth(17)\n",
    "\n",
    "        # Number of time bins in z-scored matrix (time x neurons)\n",
    "        #num_bins = dic_test[color][\"z\"].shape[0]\n",
    "\n",
    "        # Create time axis assuming bin size defined elsewhere\n",
    "        time = np.arange(-0.24, 0.5, bin_sz)\n",
    "\n",
    "        # Image limits: [x_min, x_max, y_min, y_max]\n",
    "        limits = [time[0], time[-1], 0, dic_test[color][\"z\"].shape[1]]\n",
    "\n",
    "        # Plot observed activity\n",
    "        im = ax[0].imshow(\n",
    "            np.array(dic_test[color][\"z\"]).T,  # neurons on y-axis, time on x-axis\n",
    "            aspect=\"auto\",\n",
    "            extent=limits\n",
    "        )\n",
    "        ax[0].set_title(f\"Observed {color.capitalize()}\")\n",
    "\n",
    "        # Plot predicted activity\n",
    "        im = ax[1].imshow(\n",
    "            np.array(dic_pred[color][\"z\"]).T,\n",
    "            aspect=\"auto\",\n",
    "            extent=limits\n",
    "        )\n",
    "        ax[1].set_title(f\"Predicted {color.capitalize()}\")\n",
    "\n",
    "        # Add vertical lines for stimulus onset (0s) and offset (0.25s)\n",
    "        for a in ax:\n",
    "            a.axvline(0.0, color='k', linestyle='--')\n",
    "            a.axvline(0.25, color='k', linestyle='--')\n",
    "            a.set_ylabel(\"Unit\")\n",
    "\n",
    "        # Shared x-axis label\n",
    "        fig.text(0.45, 0.00, 'Time from stim (s)', ha='center')\n",
    "\n",
    "        # Colorbar\n",
    "        fig.colorbar(im, ax=ax, location='right', label='Z-score')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e455da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zscores(dic_test, dic_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc2150",
   "metadata": {},
   "source": [
    "In our case, the predicted activity lacks much of the temporal structure seen in the observed data. This suggests that the model, which currently only includes stimulus history (black/white flash for a 250ms duration filter), may be too simple. What could we try to improve this model a little bit? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864bd89",
   "metadata": {},
   "source": [
    "## Adding coupling terms as a new predictor\n",
    "We can try extending the model in order to improve its performance. There are many ways one can do this: the iterative refinement and improvement of your model is an important part of the scientific process! In this tutorial, we’ll discuss one such extension, but you’re encouraged to try others.\n",
    "\n",
    "Now, we'll extend the model by adding coupling terms—that is, including the activity of other neurons as predictors—to account for shared variability within the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee2324",
   "metadata": {},
   "source": [
    "We start by creating a new additive basis, which will now include the spike counts of all the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04350b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New basis\n",
    "basis_coupling = nmo.basis.RaisedCosineLogConv(\n",
    "    n_basis_funcs=8, window_size=window_len, label=\"spike_history\"\n",
    ")\n",
    "\n",
    "basis = basis_white + basis_black + basis_coupling\n",
    "\n",
    "X_coupling_train = basis.compute_features(\n",
    "    predictors_train[\"white\"], \n",
    "    predictors_train[\"black\"], \n",
    "    units_counts_train\n",
    ")\n",
    "X_coupling_test = basis.compute_features(\n",
    "    predictors_test[\"white\"], \n",
    "    predictors_test[\"black\"], \n",
    "    units_counts_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc6ca0",
   "metadata": {},
   "source": [
    "We initialize a new ```PopulationGLM``` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b786d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with new basis as predictor\n",
    "# Fitting pop model\n",
    "regularizer_strength = 0.005 # Where did this come from???? We are not running this again but i also dont know how to run cross validation for a population????? and dont know how to conduct a coupling filter with single unit fit.\n",
    "\n",
    "model_coupling = nmo.glm.PopulationGLM(\n",
    "    regularizer = \"Ridge\",\n",
    "    regularizer_strength = regularizer_strength,\n",
    "    solver_name=\"LBFGS\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a197551",
   "metadata": {},
   "source": [
    "And we fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coupling.fit(X_coupling_train,units_counts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708fd7c",
   "metadata": {},
   "source": [
    "### Assess coupling GLM Population performance: heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc1f93",
   "metadata": {},
   "source": [
    "The same way as before, we can obtain the predictions using ```predict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model_coupling.predict(X_coupling_test)/ bin_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc67fe9",
   "metadata": {},
   "source": [
    "We can center unit's activity around stimuli presentation with ```compute_perievent_continuous```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35521bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perievent for predicted\n",
    "peri_white_pred = nap.compute_perievent_continuous(\n",
    "    timeseries = predicted, \n",
    "    tref = nap.Ts(flashes_test_white.start+.50),\n",
    "    minmax=(window_size)\n",
    ")  \n",
    "\n",
    "peri_black_pred =  nap.compute_perievent_continuous(\n",
    "    timeseries = predicted, \n",
    "    tref = nap.Ts(flashes_test_black.start+.50),\n",
    "    minmax=(window_size)\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4221d32",
   "metadata": {},
   "source": [
    "Create our dictionaries of z-scored mean activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd355152",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_test, dic_pred = create_zscore_dic(\n",
    "    peri_white_test,\n",
    "    peri_black_test,\n",
    "    peri_white_pred,\n",
    "    peri_black_pred\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a74d8",
   "metadata": {},
   "source": [
    "And plot our heat map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zscores(dic_test, dic_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec66bc7",
   "metadata": {},
   "source": [
    "We can see that the average peak activity is much better captured by this new model! This suggests that using time-history predictors for just these two stimuli presentation may not be sufficient for modeling neuronal activity. This makes a lot of sense! LNP models are very simple, and only having two sets of predictors is generally quite little. Our addition of coupling activity is now helping us account for non-stimuli derived variability, which improves the fit significantly and is not to be ignored!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11841330",
   "metadata": {},
   "source": [
    "## Evaluate model performance quantitatively\n",
    "\n",
    "Comparing the two models by examining their predictions is important, but you may also want a number with which to evaluate and compare your models’ performance. As discussed earlier, the GLM optimizes log-likelihood to find the best-fitting weights, and we can calculate this number using its ```score``` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0726342",
   "metadata": {},
   "source": [
    "### Using Log-Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca58945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, score_type = \"log-likelihood\"):\n",
    "    score_pop = model.score(\n",
    "        X, \n",
    "        y, \n",
    "        score_type= score_type,\n",
    "    )\n",
    "    score_unit = model.score(\n",
    "        X, \n",
    "        y, \n",
    "        aggregate_sample_scores=lambda x:np.mean(x,axis=0), \n",
    "        score_type= score_type,\n",
    "    )\n",
    "    return score_pop, score_unit \n",
    "\n",
    "def evaluate_models_by_color(models, X_sets, y_sets, flashes_color, score_type = \"log-likelihood\"):\n",
    "    '''\n",
    "    Returns\n",
    "    model base pop, model base unit, model hist pop, model hist unit\n",
    "    '''\n",
    "    models_list = []\n",
    "    for model_name, model in models.items():\n",
    "        X = X_sets[model_name].restrict(flashes_color)\n",
    "        y = y_sets.restrict(flashes_color)\n",
    "        models_list.append(evaluate_model(model, X, y, score_type))\n",
    "    return models_list[0][0], models_list[0][1], models_list[1][0],models_list[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd9ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model dictionary\n",
    "models = {\n",
    "    \"stimuli\": model_stimuli,\n",
    "    \"coupling\": model_coupling\n",
    "}\n",
    "\n",
    "score_stimuli_pop, score_stimuli_unit = evaluate_model(\n",
    "    models[\"stimuli\"],\n",
    "    X_test,\n",
    "    units_counts_test,\n",
    ")\n",
    "\n",
    "score_coupling_pop, score_coupling_unit = evaluate_model(\n",
    "    models[\"coupling\"],\n",
    "    X_coupling_test,\n",
    "    units_counts_test,\n",
    ")\n",
    "\n",
    "(score_white_stimuli_pop, \n",
    " score_white_stimuli_unit, \n",
    " score_white_coupling_pop,\n",
    " score_white_coupling_unit) = evaluate_models_by_color(\n",
    "    models,\n",
    "    {\"stimuli\": X_test, \n",
    "    \"coupling\": X_coupling_test},\n",
    "    units_counts_test,\n",
    "    flashes_test_white\n",
    ")\n",
    "\n",
    "(score_black_stimuli_pop, \n",
    " score_black_stimuli_unit, \n",
    " score_black_coupling_pop,\n",
    " score_black_coupling_unit) = evaluate_models_by_color(\n",
    "    models,\n",
    "    {\"stimuli\": X_test, \n",
    "    \"coupling\": X_coupling_test},\n",
    "    units_counts_test,\n",
    "    flashes_test_black\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3024a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_scores(scores, labels, score_method):\n",
    "    # Plot as bar chart\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    bars = plt.bar(labels, scores, color=\"skyblue\", edgecolor='black')\n",
    "\n",
    "    # Add value labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, height + 0.001, \n",
    "                f\"{height:.5f}\", ha='center', va='bottom')\n",
    "\n",
    "    plt.ylabel(f\"{score_method}\")\n",
    "    plt.title(\"Model Scores by Condition\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scalar values from single-element arrays\n",
    "scores = [\n",
    "    score_white_stimuli_pop,\n",
    "    score_white_coupling_pop,\n",
    "    score_black_stimuli_pop,\n",
    "    score_black_coupling_pop\n",
    "]\n",
    "\n",
    "# Corresponding labels for each score\n",
    "labels = [\n",
    "    \"White Stimuli\",\n",
    "    \"White + Coupling\",\n",
    "    \"Black Stimuli\",\n",
    "    \"Black + Coupling\"\n",
    "]\n",
    "\n",
    "plot_scores(\n",
    "    scores,\n",
    "    labels,\n",
    "    \"Log-likelihood\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61785e5e",
   "metadata": {},
   "source": [
    "### Using Pseudo-$R^2$ McFadden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066d3d2",
   "metadata": {},
   "source": [
    "This log-likelihood is un-normalized and thus doesn’t mean that much by itself, other than “higher=better”. When comparing alternative GLMs fit on the same dataset, whether that’s models using different regularizers and solvers or those using different predictors, comparing log-likelihoods is a reasonable thing to do.\n",
    "\n",
    "Note that, because the log-likelihood is un-normalized, it should not be compared across datasets (because e.g., it won’t account for difference in noise levels). We provide the ability to compute the pseudo-$R^2$ for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecf348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model dictionary\n",
    "models = {\n",
    "    \"stimuli\": model_stimuli,\n",
    "    \"coupling\": model_coupling\n",
    "}\n",
    "\n",
    "score_stimuli_pop, score_stimuli_unit = evaluate_model(\n",
    "    models[\"stimuli\"],\n",
    "    X_test,\n",
    "    units_counts_test,\n",
    ")\n",
    "\n",
    "score_coupling_pop, score_coupling_unit = evaluate_model(\n",
    "    models[\"coupling\"],\n",
    "    X_coupling_test,\n",
    "    units_counts_test,\n",
    "    \"pseudo-r2-McFadden\"\n",
    ")\n",
    "\n",
    "(score_white_stimuli_pop, \n",
    " score_white_stimuli_unit, \n",
    " score_white_coupling_pop,\n",
    " score_white_coupling_unit) = evaluate_models_by_color(\n",
    "    models,\n",
    "    {\"stimuli\": X_test, \n",
    "    \"coupling\": X_coupling_test},\n",
    "    units_counts_test,\n",
    "    flashes_test_white,\n",
    "    \"pseudo-r2-McFadden\"\n",
    ")\n",
    "\n",
    "(score_black_stimuli_pop, \n",
    " score_black_stimuli_unit, \n",
    " score_black_coupling_pop,\n",
    " score_black_coupling_unit) = evaluate_models_by_color(\n",
    "    models,\n",
    "    {\"stimuli\": X_test, \n",
    "    \"coupling\": X_coupling_test},\n",
    "    units_counts_test,\n",
    "    flashes_test_black,\n",
    "    \"pseudo-r2-McFadden\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scalar values from single-element arrays\n",
    "scores = [\n",
    "    score_white_stimuli_pop,\n",
    "    score_white_coupling_pop,\n",
    "    score_black_stimuli_pop,\n",
    "    score_black_coupling_pop\n",
    "]\n",
    "\n",
    "# Corresponding labels for each score\n",
    "labels = [\n",
    "    \"White Stimuli\",\n",
    "    \"White + Coupling\",\n",
    "    \"Black Stimuli\",\n",
    "    \"Black + Coupling\"\n",
    "]\n",
    "\n",
    "plot_scores(\n",
    "    scores,\n",
    "    labels,\n",
    "    \"Pseudo-$R^2$ McFadden\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3635c",
   "metadata": {},
   "source": [
    "We can also see the individual scores for each unit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e85f7c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_unit_scores():\n",
    "        # Combine all arrays into a list\n",
    "    score_arrays = [\n",
    "        score_white_stimuli_unit,\n",
    "        score_white_coupling_unit,\n",
    "        score_black_stimuli_unit,\n",
    "        score_black_coupling_unit\n",
    "    ]\n",
    "\n",
    "    labels = [\"White Stimuli\", \"White + Cupling\", \"Black Stimuli\", \"Black + Coupling\"]\n",
    "    num_conditions = len(score_arrays)\n",
    "    num_units = len(score_white_stimuli_unit)\n",
    "\n",
    "    # X positions for boxplots (centered)\n",
    "    x_positions = np.arange(num_conditions)\n",
    "    plt.figure(figsize=(17, 5))\n",
    "\n",
    "    # Plot boxplots\n",
    "    plt.boxplot(score_arrays, positions=x_positions, widths=0.5)\n",
    "\n",
    "    # Scatter individual dots (jittered) and connect lines\n",
    "    for unit_idx in range(num_units):\n",
    "        unit_scores = [arr[unit_idx] for arr in score_arrays]\n",
    "        x_jittered = x_positions + np.random.normal(scale=0.01, size=num_conditions)\n",
    "        \n",
    "        # Scatter dots\n",
    "        plt.scatter(x_jittered, unit_scores, color=\"black\", s=10, alpha=0.7)\n",
    "\n",
    "        # Connect dots for this unit\n",
    "        plt.plot(x_jittered, unit_scores, color=\"black\", alpha=0.7, linewidth=1)\n",
    "\n",
    "    # X-axis labels\n",
    "    plt.xticks(x_positions, labels)\n",
    "    plt.ylabel(\"Pseudo-$R^2$ McFadden Score\")\n",
    "    plt.title(\"Unit scores across models and conditions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945235bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unit_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb57cd",
   "metadata": {},
   "source": [
    "Admonition\n",
    "\n",
    "Now we want to see which model has the best performance. When assesing this, it is important that we ask ourselves what is the most reasonable way to do it.\n",
    "\n",
    "Usually, in linear regression, the $Rˆ2$ measure is used, which is interpreted as the proportion of explained variance by a given model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R^2 = 1- \\frac{\\sum_i{(y_i - \\hat{y})^2}}{\\sum_i{(y_i - \\mu)^2}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The numerator in this equation is the variance NOT explained by the model, while the denominator is the total variance of the model. For example, if a given model is perfect at predicting the mean but nothing else, then:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R^2 = 1- \\frac{\\sum_i{(y_i - \\hat{y})^2}}{\\sum_i{(y_i - \\mu)^2}} = 1 - \\frac{\\sum_i{(y_i - \\mu)^2}}{\\sum_i{(y_i - \\mu)^2}} = 1 - 1 = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and the model explains very little of the variance.\n",
    "\n",
    "Crucially, however, in GLMs, the variance has some characteristics that are incompatible with regular linear regression. In particular:\n",
    "1. GLMs do not meet the homoscedasticity assumption necessary for linear regression (and $Rˆ2$ interpretation) to make sense. This assumption describes a situation in which the error term is the same across all samples; i.e. constant variance. In GLMs, the variance is not constant! In particular with LNP models, the variance is actually the same as the mean!\n",
    "2. The $R^2$ captures the variance explained when there is a linear relationship between the observations and the predictor. However, in GLMs, the non-linearity sets a non-linear mapping between the predictors and the mean of the observations. Because of that, the variance explained interpretation does not hold either.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34f226",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e5ff0",
   "metadata": {},
   "source": [
    "Questions\n",
    "- scores plot per unit looks awful\n",
    "- basis convolution plot looks muy amplio? ask eduardo\n",
    "\n",
    "To fix now:\n",
    "- Put in peth text that im showing first 10 neurons\n",
    "- Complete quantitative evaluation texts\n",
    "- Read all texts\n",
    "- Add references/citations\n",
    "- check all titles are uniform in terms of caps\n",
    "- Make sure there are explanations in markdown for all steps\n",
    "- Comment all functions\n",
    "- Update requirements and check if passess tests\n",
    "- is making the window smaller better? - 100ms\n",
    "- add link to additive basis\n",
    "\n",
    "Optional:\n",
    "- Add more resources\n",
    "- Add estimated time of completion\n",
    "- Add learning objectives\n",
    "- plot peth one by one for output of models?\n",
    "- Add self-reflection questions (ideas - limitations for this model?)\n",
    "- plot basis dot coefficient for a neuron to see learned filter in action\n",
    "- add single neuron plot for perievent predicted no smoothing\n",
    "\n",
    " \n",
    "Optional if decide to show responsiveness & coupled filters:\n",
    "- convolve coefficients with basis and sum that? - also, im doing something wrong with the plot. double check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90450b90",
   "metadata": {},
   "source": [
    "## Other stuff I will prolly not use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da0166",
   "metadata": {},
   "source": [
    "### Interpreting the coefficients?\n",
    "The learned model coefficients are in a 2D array of shape ```(n_predictors, n_neurons)```. This array concatenates the coefficients representing pairwise couplings along the first dimension. Using the ```split_by_feature``` method of the basis object, the coefficients can be reshaped into a 3D array of shape ```(n_neurons, n_basis_funcs, n_neurons```, where the first dimension corresponds to sender neurons, the second dimension contains the basis function coefficients, and the third dimension corresponds to receiver neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2933db2",
   "metadata": {},
   "source": [
    "Let's select the neurons for which the history filter was actually better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcace6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dictionary (1 key per basis component, here there is single basis component)\n",
    "weights_dict = basis.split_by_feature(model_coupling.coef_, axis=0)\n",
    "\n",
    "weights = weights_dict[\"spike_history\"]\n",
    "\n",
    "weights_sum = weights.sum(axis=1)\n",
    "\n",
    "weights_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd803ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_white = score_white_coupling_unit - score_white_stimuli_unit\n",
    "diff_black = score_black_coupling_unit -  score_black_stimuli_unit\n",
    "\n",
    "mask_white = (diff_white > 0.05).astype(int)\n",
    "mask_black = (diff_black > 0.05).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_bool = mask_white.astype(bool)  # ensure it's boolean\n",
    "\n",
    "# Select only columns (i.e., select target neurons)\n",
    "weights_selected = weights_sum[:, mask_bool]\n",
    "weights_selected.shape\n",
    "\n",
    "\n",
    "# Selected pairs\n",
    "neuron_ids = list(units.index)  \n",
    "\n",
    "mask_bool = mask_white.astype(bool)\n",
    "\n",
    "# Apply mask to neuron_ids\n",
    "masked_neuron_ids = [nid for nid, keep in zip(neuron_ids, mask_bool) if keep]\n",
    "\n",
    "# Generate all pairs from masked neurons\n",
    "pairs = [(s, r) for s, r in itertools.product(neuron_ids, masked_neuron_ids)]\n",
    "\n",
    "pairs_ = [(s, r) for s, r in itertools.product(list(range(weights_selected.shape[0])),list(range(weights_selected.shape[1])))]\n",
    "\n",
    "# Get responsiveness values\n",
    "stimulus = \"responsiveness_white\"\n",
    "x = [units[[s]][stimulus].values[0] for s, r in pairs]\n",
    "y = [units[[r]][stimulus].values[0] for s, r in pairs]\n",
    "\n",
    "idx_to_unit = list(units.index)\n",
    "unit_to_idx = {unit: i for i, unit in enumerate(idx_to_unit)}\n",
    "\n",
    "# Color: summed weights from spike history\n",
    "colors = [weights_sum[s, r] for s, r in pairs_]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 6))\n",
    "sc = plt.scatter(x, y, c=colors, cmap=\"RdYlBu\", alpha=0.7, s=30)\n",
    "plt.xlabel(\"Sender neuron responsiveness\")\n",
    "plt.ylabel(\"Receiver neuron responsiveness\")\n",
    "plt.title(\"Sender vs Receiver Responsiveness (Color: Spike History Weights Sum)\")\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label(\"∑ Spike History Weights\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_bool = mask_black.astype(bool)  # ensure it's boolean\n",
    "\n",
    "# Select only columns (i.e., select target neurons)\n",
    "weights_selected = weights_sum[:, mask_bool]\n",
    "weights_selected.shape\n",
    "\n",
    "\n",
    "# Selected pairs\n",
    "neuron_ids = list(units.index)  \n",
    "\n",
    "mask_bool = mask_black.astype(bool)\n",
    "\n",
    "# Apply mask to neuron_ids\n",
    "masked_neuron_ids = [nid for nid, keep in zip(neuron_ids, mask_bool) if keep]\n",
    "\n",
    "# Generate all pairs from masked neurons\n",
    "pairs = [(s, r) for s, r in itertools.product(neuron_ids, masked_neuron_ids)]\n",
    "\n",
    "\n",
    "pairs_ = [(s, r) for s, r in itertools.product(list(range(weights_selected.shape[0])),list(range(weights_selected.shape[1])))]\n",
    "\n",
    "# Get responsiveness values\n",
    "stimulus = \"responsiveness_black\"\n",
    "x = [units[[s]][stimulus].values[0] for s, r in pairs]\n",
    "y = [units[[r]][stimulus].values[0] for s, r in pairs]\n",
    "\n",
    "idx_to_unit = list(units.index)\n",
    "unit_to_idx = {unit: i for i, unit in enumerate(idx_to_unit)}\n",
    "\n",
    "# Color: summed weights from spike history\n",
    "colors = [weights_sum[s, r] for s, r in pairs_]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 6))\n",
    "sc = plt.scatter(x, y, c=colors, cmap=\"RdYlBu\", alpha=0.7, s=30)\n",
    "plt.xlabel(\"Sender neuron responsiveness\")\n",
    "plt.ylabel(\"Receiver neuron responsiveness\")\n",
    "plt.title(\"Sender vs Receiver Responsiveness (Color: Spike History Weights Sum)\")\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label(\"∑ Spike History Weights\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d04a66f",
   "metadata": {},
   "source": [
    "note on intercept:\n",
    "(for the GLM object, this will always be 1, but it will differ for the PopulationGLM object!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9ac07",
   "metadata": {},
   "source": [
    "### Extra: fitting single neuron spiking activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Fitting single unit\n",
    "reg_strength = 0.005\n",
    "models = np.empty((units_counts_train.shape[1]), dtype=object) # as many fits as units\n",
    "\n",
    "predictions = np.empty((units_counts_test.shape[1]), dtype=object) # test timepoings x units\n",
    "#num_units = units_counts_test.shape[1]\n",
    "#predictions = {unit: None for unit in range(num_units)}\n",
    "\n",
    "for unit in range(units_counts_test.shape[1]):\n",
    "    # New basis\n",
    "    spike_hist_bas = nmo.basis.RaisedCosineLogConv(\n",
    "        n_basis_funcs=8, window_size=window_len, label=\"spike_history\"\n",
    "    )\n",
    "\n",
    "    basis = additive_basis + spike_hist_bas\n",
    "\n",
    "    X_coupling_train = basis.compute_features(\n",
    "        predictors_train[\"white\"], \n",
    "        predictors_train[\"black\"], \n",
    "        units_counts_train[:,unit]\n",
    "    )\n",
    "    X_coupling_test = basis.compute_features(\n",
    "        predictors_test[\"white\"], \n",
    "        predictors_test[\"black\"], \n",
    "        units_counts_test[:,unit]\n",
    "    )\n",
    "    model = nmo.glm.GLM(\n",
    "        regularizer = \"Ridge\",\n",
    "        regularizer_strength = reg_strength,\n",
    "        solver_name=\"LBFGS\"\n",
    "    )\n",
    "    \n",
    "    model.fit(X_coupling_train,units_counts_train[:,unit])\n",
    "    model.score(X_coupling_test,units_counts_test[:,unit])\n",
    "\n",
    "    predictions[unit] = model.predict(X_coupling_test)/ bin_sz\n",
    "    models[unit] = model\n",
    "\n",
    "n_features =18\n",
    "n_neurons = units_counts_train.shape[1]\n",
    "coef_ = np.zeros((n_features, n_neurons))\n",
    "intercept_ = np.zeros(n_neurons)\n",
    "\n",
    "for neuron in range(units_counts_test.shape[1]):\n",
    "    coef_[:,neuron] = models[neuron].coef_\n",
    "    intercept_[neuron] = models[neuron].intercept_[0]\n",
    "\n",
    "pop_model_hist = nmo.glm.PopulationGLM()\n",
    "\n",
    "pop_model_hist.coef_ = coef_\n",
    "pop_model_hist.intercept_ = intercept_\n",
    "\n",
    "# Assuming predictions is of shape (47,)\n",
    "# and each predictions[i].as_array() is (120000,)\n",
    "arr_list = [predictions[i].as_array() for i in range(len(predictions))]\n",
    "\n",
    "# Shape: (120000, 47)\n",
    "pred_arr = np.column_stack(arr_list)\n",
    "\n",
    "tsframe = nap.TsdFrame(t = units_counts_test.t, d = pred_arr)\n",
    "tsframe\n",
    "\n",
    "predicted = tsframe\n",
    "\n",
    "# Calculate perievent for predicted\n",
    "peri_white_pred = nap.compute_perievent_continuous(\n",
    "    timeseries = predicted, \n",
    "    tref = nap.Ts(flashes_test_white.start+.50),\n",
    "    minmax=(window_size)\n",
    ")  \n",
    "\n",
    "peri_black_pred =  nap.compute_perievent_continuous(\n",
    "    timeseries = predicted, \n",
    "    tref = nap.Ts(flashes_test_black.start+.50),\n",
    "    minmax=(window_size)\n",
    ")  \n",
    "\n",
    "dic_test, dic_pred = create_zscore_dic(\n",
    "    peri_white_test,\n",
    "    peri_black_test,\n",
    "    peri_white_pred,\n",
    "    peri_black_pred\n",
    ")\n",
    "\n",
    "plot_zscores(dic_test, dic_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e156f2",
   "metadata": {},
   "source": [
    "Not as good as coupling filter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c8f19e",
   "metadata": {},
   "source": [
    ":::{admonition} This is a different regularizer_strenght value - Where did that come from?\n",
    ":class: info\n",
    ":class: dropdown\n",
    "\n",
    "We conducted cross validation again to obtain the regularization strength for this new model. Adding extra parameters increases the risk of overfitting, so getting a new regularization_strength to account for that makes sense.\n",
    "\n",
    "```\n",
    "spike_hist_bas = nmo.basis.RaisedCosineLogConv(\n",
    "    n_basis_funcs=8, window_size=window_len, label=\"spike_history\"\n",
    ")\n",
    "\n",
    "basis = additive_basis + spike_hist_bas\n",
    "\n",
    "X_hist_train = basis.compute_features(\n",
    "        predictors_train[\"white\"], \n",
    "        predictors_train[\"black\"], \n",
    "        u_counts_train\n",
    "    )\n",
    "X_hist_test = basis.compute_features(\n",
    "    predictors_test[\"white\"], \n",
    "    predictors_test[\"black\"], \n",
    "    u_counts_test\n",
    ")\n",
    "model = nmo.glm.GLM(\n",
    "    regularizer = \"Ridge\",\n",
    "    regularizer_strength = 0.0001,\n",
    "    solver_name=\"LBFGS\"\n",
    ")\n",
    "# Create parameter grid\n",
    "param_grid = {\n",
    "    \"regularizer_strength\" : \n",
    "    np.geomspace(10**-9, 10, 10)\n",
    "}\n",
    "\n",
    "# Instantiate the grid search object\n",
    "grid_search = GridSearchCV(\n",
    "    model,param_grid, \n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "# Run grid search\n",
    "grid_search.fit(X_hist_train, u_counts_train)\n",
    "\n",
    "# Print optimal parameter\n",
    "print(grid_search.best_estimator_.regularizer_strength)\n",
    ">>> 0.004641588833612782\n",
    "```\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e506e",
   "metadata": {},
   "source": [
    ":::{admonition} How can PopulationGLM be equivalent but faster?\n",
    "bla\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69814b53",
   "metadata": {},
   "source": [
    "add an admonition on why there is no prediction prior to the presentation of stimulus\n",
    "admonition\n",
    "taking an extra window because of the convolution thingy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11c421",
   "metadata": {},
   "source": [
    "Maybe not include: We will also show how, if you have recordings from a large population of neurons simultaneously, you can build connections between the neurons into the GLM in the form of coupling filters. This can help answer the degree to which activity is driven primarily by the input X, or by network influences in the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to import if you saved the data\n",
    "#path = \"sub-726298249_ses-754829445.nwb\"\n",
    "#data = nap.load_file(path)\n",
    "#nwb = data.nwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f01e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c41ea6d1",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "- What happens if you change test and train set to pick different trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde86787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to import if you saved the data\n",
    "path = \"sub-726298249_ses-754829445.nwb\"\n",
    "data = nap.load_file(path)\n",
    "nwb = data.nwb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
